{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_key</th>\n",
       "      <th>race_date</th>\n",
       "      <th>dr</th>\n",
       "      <th>distance</th>\n",
       "      <th>field_going</th>\n",
       "      <th>course_type</th>\n",
       "      <th>race_money</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>declare_horse_wt</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>horse</th>\n",
       "      <th>jockey</th>\n",
       "      <th>trainer</th>\n",
       "      <th>is_champ</th>\n",
       "      <th>pla</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>好地至快地</td>\n",
       "      <td>草地</td>\n",
       "      <td>575000</td>\n",
       "      <td>120</td>\n",
       "      <td>1186</td>\n",
       "      <td>7.3</td>\n",
       "      <td>有情風(N139)</td>\n",
       "      <td>薛寶力</td>\n",
       "      <td>吳定強</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.37</td>\n",
       "      <td>17.430713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>好地至快地</td>\n",
       "      <td>草地</td>\n",
       "      <td>575000</td>\n",
       "      <td>132</td>\n",
       "      <td>1022</td>\n",
       "      <td>6.1</td>\n",
       "      <td>樂趣(S150)</td>\n",
       "      <td>韋達</td>\n",
       "      <td>霍利時</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>57.53</td>\n",
       "      <td>17.382235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>好地至快地</td>\n",
       "      <td>草地</td>\n",
       "      <td>575000</td>\n",
       "      <td>121</td>\n",
       "      <td>1085</td>\n",
       "      <td>48.0</td>\n",
       "      <td>穩佔先機(N359)</td>\n",
       "      <td>連達文</td>\n",
       "      <td>苗禮德</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>57.78</td>\n",
       "      <td>17.307027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "      <td>好地至快地</td>\n",
       "      <td>草地</td>\n",
       "      <td>575000</td>\n",
       "      <td>127</td>\n",
       "      <td>1211</td>\n",
       "      <td>7.8</td>\n",
       "      <td>上浦勇將(P285)</td>\n",
       "      <td>田泰安</td>\n",
       "      <td>徐雨石</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57.82</td>\n",
       "      <td>17.295054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>好地至快地</td>\n",
       "      <td>草地</td>\n",
       "      <td>575000</td>\n",
       "      <td>124</td>\n",
       "      <td>1088</td>\n",
       "      <td>14.0</td>\n",
       "      <td>大地王者(L251)</td>\n",
       "      <td>黎海榮</td>\n",
       "      <td>李易達</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>57.89</td>\n",
       "      <td>17.274141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       race_key   race_date  dr  distance field_going course_type  race_money  \\\n",
       "0  2015/04/22_1  2015/04/22   2      1000       好地至快地          草地      575000   \n",
       "1  2015/04/22_1  2015/04/22   3      1000       好地至快地          草地      575000   \n",
       "2  2015/04/22_1  2015/04/22   5      1000       好地至快地          草地      575000   \n",
       "3  2015/04/22_1  2015/04/22   7      1000       好地至快地          草地      575000   \n",
       "4  2015/04/22_1  2015/04/22   1      1000       好地至快地          草地      575000   \n",
       "\n",
       "   act_wt  declare_horse_wt  win_odds       horse jockey trainer  is_champ  \\\n",
       "0     120              1186       7.3   有情風(N139)    薛寶力     吳定強         1   \n",
       "1     132              1022       6.1    樂趣(S150)     韋達     霍利時         0   \n",
       "2     121              1085      48.0  穩佔先機(N359)    連達文     苗禮德         0   \n",
       "3     127              1211       7.8  上浦勇將(P285)    田泰安     徐雨石         0   \n",
       "4     124              1088      14.0  大地王者(L251)    黎海榮     李易達         0   \n",
       "\n",
       "   pla  finish_time      speed  \n",
       "0    1        57.37  17.430713  \n",
       "1    2        57.53  17.382235  \n",
       "2    3        57.78  17.307027  \n",
       "3    4        57.82  17.295054  \n",
       "4    5        57.89  17.274141  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "perform = pd.read_csv('./horse/data/perform_clean.csv', sep=',', encoding='utf-8')\n",
    "perform['speed'] = perform['distance']/perform['finish_time']\n",
    "perform['is_champ'] = perform['pla'].apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "y_cols = ['is_champ', 'pla', 'finish_time', 'speed']\n",
    "date_cols = ['race_key', 'race_date']\n",
    "\n",
    "get_x_cols = lambda x: [col for col in x if (col not in y_cols) and (col not in date_cols)]\n",
    "x_cols = get_x_cols(perform.columns)\n",
    "\n",
    "perform.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Feature Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_key</th>\n",
       "      <th>race_date</th>\n",
       "      <th>dr</th>\n",
       "      <th>distance</th>\n",
       "      <th>field_going</th>\n",
       "      <th>course_type</th>\n",
       "      <th>race_money</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>declare_horse_wt</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>horse</th>\n",
       "      <th>jockey</th>\n",
       "      <th>trainer</th>\n",
       "      <th>is_champ</th>\n",
       "      <th>pla</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>575000</td>\n",
       "      <td>120</td>\n",
       "      <td>1186</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.37</td>\n",
       "      <td>17.430713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>575000</td>\n",
       "      <td>132</td>\n",
       "      <td>1022</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>57.53</td>\n",
       "      <td>17.382235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>575000</td>\n",
       "      <td>121</td>\n",
       "      <td>1085</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>57.78</td>\n",
       "      <td>17.307027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>575000</td>\n",
       "      <td>127</td>\n",
       "      <td>1211</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57.82</td>\n",
       "      <td>17.295054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>575000</td>\n",
       "      <td>124</td>\n",
       "      <td>1088</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>57.89</td>\n",
       "      <td>17.274141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       race_key   race_date  dr  distance  field_going  course_type  \\\n",
       "0  2015/04/22_1  2015/04/22   2      1000            0            1   \n",
       "1  2015/04/22_1  2015/04/22   3      1000            0            1   \n",
       "2  2015/04/22_1  2015/04/22   5      1000            0            1   \n",
       "3  2015/04/22_1  2015/04/22   7      1000            0            1   \n",
       "4  2015/04/22_1  2015/04/22   1      1000            0            1   \n",
       "\n",
       "   race_money  act_wt  declare_horse_wt  win_odds  horse  jockey  trainer  \\\n",
       "0      575000     120              1186       7.3      0       0        0   \n",
       "1      575000     132              1022       6.1      1       1        1   \n",
       "2      575000     121              1085      48.0      2       2        2   \n",
       "3      575000     127              1211       7.8      3       3        3   \n",
       "4      575000     124              1088      14.0      4       4        4   \n",
       "\n",
       "   is_champ  pla  finish_time      speed  \n",
       "0         1    1        57.37  17.430713  \n",
       "1         0    2        57.53  17.382235  \n",
       "2         0    3        57.78  17.307027  \n",
       "3         0    4        57.82  17.295054  \n",
       "4         0    5        57.89  17.274141  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low cardinality, encoding with 0/1\n",
    "perform.course_type.unique() # '草地', '全天候跑道'; nunique=2\n",
    "perform.course_type = perform.course_type.replace({'全天候跑道':0, '草地':1})\n",
    "\n",
    "# high cardinality, encoding with embedding. \n",
    "#   Note: Emb value is in model, here only process with mapping index\n",
    "def col2ix(df_col:pd.Series):\n",
    "    \"\"\" Mapping from column's item to indecies.\n",
    "\n",
    "        Returns:\n",
    "            ix2item: dict\n",
    "                with key as index, value is the hashed item\n",
    "            item2ix: dict\n",
    "                with key as item, value as its index\n",
    "    \"\"\"\n",
    "    ix = list(range(df_col.nunique()))\n",
    "    item = df_col.unique().tolist()\n",
    "\n",
    "    return dict(zip(ix, item)), dict(zip(item, ix))\n",
    "\n",
    "# mapping dict\n",
    "ix2field, field2ix = col2ix(perform['field_going'])\n",
    "ix2jockey, jockey2ix = col2ix(perform['jockey'])\n",
    "ix2horse, horse2ix = col2ix(perform['horse'])\n",
    "ix2trainer, trainer2ix = col2ix(perform['trainer'])\n",
    "\n",
    "# perform after mapping\n",
    "perform['field_going'] = perform['field_going'].apply(lambda x: field2ix[x])\n",
    "perform['jockey'] = perform['jockey'].apply(lambda x: jockey2ix[x])\n",
    "perform['horse'] = perform['horse'].apply(lambda x: horse2ix[x])\n",
    "perform['trainer'] = perform['trainer'].apply(lambda x: trainer2ix[x])\n",
    "\n",
    "perform.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_key</th>\n",
       "      <th>race_date</th>\n",
       "      <th>dr</th>\n",
       "      <th>distance</th>\n",
       "      <th>field_going</th>\n",
       "      <th>course_type</th>\n",
       "      <th>race_money</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>declare_horse_wt</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>horse</th>\n",
       "      <th>jockey</th>\n",
       "      <th>trainer</th>\n",
       "      <th>is_champ</th>\n",
       "      <th>pla</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>-0.540745</td>\n",
       "      <td>1.080522</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.37</td>\n",
       "      <td>17.430713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>1.432011</td>\n",
       "      <td>-1.417146</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>57.53</td>\n",
       "      <td>17.382235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>-0.376349</td>\n",
       "      <td>-0.457676</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>57.78</td>\n",
       "      <td>17.307027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>0.610030</td>\n",
       "      <td>1.461264</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57.82</td>\n",
       "      <td>17.295054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>0.116841</td>\n",
       "      <td>-0.411987</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>57.89</td>\n",
       "      <td>17.274141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       race_key   race_date  dr  distance  field_going  course_type  \\\n",
       "0  2015/04/22_1  2015/04/22   2 -1.404898            0            1   \n",
       "1  2015/04/22_1  2015/04/22   3 -1.404898            0            1   \n",
       "2  2015/04/22_1  2015/04/22   5 -1.404898            0            1   \n",
       "3  2015/04/22_1  2015/04/22   7 -1.404898            0            1   \n",
       "4  2015/04/22_1  2015/04/22   1 -1.404898            0            1   \n",
       "\n",
       "   race_money    act_wt  declare_horse_wt  win_odds  horse  jockey  trainer  \\\n",
       "0   -0.394974 -0.540745          1.080522       7.3      0       0        0   \n",
       "1   -0.394974  1.432011         -1.417146       6.1      1       1        1   \n",
       "2   -0.394974 -0.376349         -0.457676      48.0      2       2        2   \n",
       "3   -0.394974  0.610030          1.461264       7.8      3       3        3   \n",
       "4   -0.394974  0.116841         -0.411987      14.0      4       4        4   \n",
       "\n",
       "   is_champ  pla  finish_time      speed  \n",
       "0         1    1        57.37  17.430713  \n",
       "1         0    2        57.53  17.382235  \n",
       "2         0    3        57.78  17.307027  \n",
       "3         0    4        57.82  17.295054  \n",
       "4         0    5        57.89  17.274141  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zscore_standarlization(col):\n",
    "    return (col-col.mean())/col.std()\n",
    "\n",
    "perform['distance'] = zscore_standarlization(perform['distance'])\n",
    "perform['race_money'] = zscore_standarlization(perform['race_money'])\n",
    "perform['act_wt'] = zscore_standarlization(perform['act_wt'])\n",
    "perform['declare_horse_wt'] = zscore_standarlization(perform['declare_horse_wt'])\n",
    "\n",
    "perform.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform.distance.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54436, 17), (43157, 17), (5558, 17), (5721, 17))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from horse.process import train_test_split\n",
    "\n",
    "perform_train, perform_val, perform_test = train_test_split(perform, 'race_date', [0.8, 0.1, 0.1])\n",
    "\n",
    "perform.shape, perform_train.shape, perform_val.shape, perform_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_key</th>\n",
       "      <th>race_date</th>\n",
       "      <th>dr</th>\n",
       "      <th>distance</th>\n",
       "      <th>field_going</th>\n",
       "      <th>course_type</th>\n",
       "      <th>race_money</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>declare_horse_wt</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>horse</th>\n",
       "      <th>jockey</th>\n",
       "      <th>trainer</th>\n",
       "      <th>is_champ</th>\n",
       "      <th>pla</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>speed</th>\n",
       "      <th>dr_ix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>-0.540745</td>\n",
       "      <td>1.080522</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.37</td>\n",
       "      <td>17.430713</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>1.432011</td>\n",
       "      <td>-1.417146</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>57.53</td>\n",
       "      <td>17.382235</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>-0.376349</td>\n",
       "      <td>-0.457676</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>57.78</td>\n",
       "      <td>17.307027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>0.610030</td>\n",
       "      <td>1.461264</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>57.82</td>\n",
       "      <td>17.295054</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/04/22</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.394974</td>\n",
       "      <td>0.116841</td>\n",
       "      <td>-0.411987</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>57.89</td>\n",
       "      <td>17.274141</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54431</th>\n",
       "      <td>2023/01/18_8</td>\n",
       "      <td>2023/01/18</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103894</td>\n",
       "      <td>-0.540745</td>\n",
       "      <td>-0.807959</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4203</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>57.51</td>\n",
       "      <td>17.388280</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54432</th>\n",
       "      <td>2023/01/18_8</td>\n",
       "      <td>2023/01/18</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103894</td>\n",
       "      <td>-0.705141</td>\n",
       "      <td>1.628791</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3364</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>57.66</td>\n",
       "      <td>17.343045</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54433</th>\n",
       "      <td>2023/01/18_8</td>\n",
       "      <td>2023/01/18</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103894</td>\n",
       "      <td>-0.376349</td>\n",
       "      <td>-0.640432</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2870</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>57.78</td>\n",
       "      <td>17.307027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54434</th>\n",
       "      <td>2023/01/18_8</td>\n",
       "      <td>2023/01/18</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103894</td>\n",
       "      <td>-0.869538</td>\n",
       "      <td>1.217589</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4270</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>58.34</td>\n",
       "      <td>17.140898</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54435</th>\n",
       "      <td>2023/01/18_8</td>\n",
       "      <td>2023/01/18</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.404898</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103894</td>\n",
       "      <td>-0.376349</td>\n",
       "      <td>-0.107393</td>\n",
       "      <td>115.0</td>\n",
       "      <td>4362</td>\n",
       "      <td>118</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>58.90</td>\n",
       "      <td>16.977929</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54436 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           race_key   race_date  dr  distance  field_going  course_type  \\\n",
       "0      2015/04/22_1  2015/04/22   2 -1.404898            0            1   \n",
       "1      2015/04/22_1  2015/04/22   3 -1.404898            0            1   \n",
       "2      2015/04/22_1  2015/04/22   5 -1.404898            0            1   \n",
       "3      2015/04/22_1  2015/04/22   7 -1.404898            0            1   \n",
       "4      2015/04/22_1  2015/04/22   1 -1.404898            0            1   \n",
       "...             ...         ...  ..       ...          ...          ...   \n",
       "54431  2023/01/18_8  2023/01/18   6 -1.404898            1            1   \n",
       "54432  2023/01/18_8  2023/01/18  11 -1.404898            1            1   \n",
       "54433  2023/01/18_8  2023/01/18   3 -1.404898            1            1   \n",
       "54434  2023/01/18_8  2023/01/18   8 -1.404898            1            1   \n",
       "54435  2023/01/18_8  2023/01/18   7 -1.404898            1            1   \n",
       "\n",
       "       race_money    act_wt  declare_horse_wt  win_odds  horse  jockey  \\\n",
       "0       -0.394974 -0.540745          1.080522       7.3      0       0   \n",
       "1       -0.394974  1.432011         -1.417146       6.1      1       1   \n",
       "2       -0.394974 -0.376349         -0.457676      48.0      2       2   \n",
       "3       -0.394974  0.610030          1.461264       7.8      3       3   \n",
       "4       -0.394974  0.116841         -0.411987      14.0      4       4   \n",
       "...           ...       ...               ...       ...    ...     ...   \n",
       "54431    0.103894 -0.540745         -0.807959      35.0   4203      36   \n",
       "54432    0.103894 -0.705141          1.628791      36.0   3364      21   \n",
       "54433    0.103894 -0.376349         -0.640432      17.0   2870      12   \n",
       "54434    0.103894 -0.869538          1.217589     144.0   4270       4   \n",
       "54435    0.103894 -0.376349         -0.107393     115.0   4362     118   \n",
       "\n",
       "       trainer  is_champ  pla  finish_time      speed  dr_ix  \n",
       "0            0         1    1        57.37  17.430713      0  \n",
       "1            1         0    2        57.53  17.382235      1  \n",
       "2            2         0    3        57.78  17.307027      2  \n",
       "3            3         0    4        57.82  17.295054      3  \n",
       "4            4         0    5        57.89  17.274141      4  \n",
       "...        ...       ...  ...          ...        ...    ...  \n",
       "54431        8         0    8        57.51  17.388280      5  \n",
       "54432        3         0    9        57.66  17.343045      6  \n",
       "54433       12         0   10        57.78  17.307027      1  \n",
       "54434       15         0   11        58.34  17.140898      7  \n",
       "54435       20         0   12        58.90  16.977929      3  \n",
       "\n",
       "[54436 rows x 18 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from horse.data.load_data import DataSet\n",
    "\n",
    "root = './horse/data/perform_clean.csv'\n",
    "scaling = True\n",
    "\n",
    "data = DataSet(root, scaling)\n",
    "data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37530, 18), (11185, 18), (5721, 18))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, test = data.train_val_test_spliting(perc=[0.7, 0.2, 0.1])\n",
    "\n",
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Assumption 1\n",
    "\n",
    "Assume that all horses are independently racing at the court, with no affect regarding the number of race it is taking.\n",
    "\n",
    "It is safe to treat it as a binary classification problem in this case, and apply the binary cross entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor\n",
    "\n",
    "y_col = ['is_champ']\n",
    "numerical_cols = ['distance', 'course_type', 'race_money', 'act_wt', 'declare_horse_wt', 'dr']\n",
    "categ_cols = ['field_going', 'horse', 'jockey', 'trainer']\n",
    "x_cols = numerical_cols + categ_cols\n",
    "\n",
    "def get_feats(data, numerical_cols, y_col, use_cuda=False):\n",
    "    compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "    num = FloatTensor(data[numerical_cols].values).to(compute_device)\n",
    "    d = LongTensor(data['dr_ix'].values).to(compute_device)\n",
    "    f = LongTensor(data['field_going'].values).to(compute_device)\n",
    "    j = LongTensor(data['jockey'].values).to(compute_device)\n",
    "    h = LongTensor(data['horse'].values).to(compute_device)\n",
    "    t = LongTensor(data['trainer'].values).to(compute_device)\n",
    "    y = FloatTensor(data[y_col].values).to(compute_device)\n",
    "\n",
    "    X = (num, d, f, j, h, t)\n",
    "    return X, y\n",
    "\n",
    "def horse_data_loader(X, y, batch_size, shuffle):\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    return DataLoader(list(zip(*X, y)), batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def create_data_batch(data, numerical_cols, y_col, batch_size, shuffle=True):\n",
    "    X, y = get_feats(data, numerical_cols, y_col)\n",
    "\n",
    "    return horse_data_loader(X, y, batch_size, shuffle)\n",
    "\n",
    "dl = create_data_batch(perform_train, numerical_cols, y_col, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from horse.process import racing_champ, AveragePrecision\n",
    "\n",
    "def prep_eval_data(perform, use_cuda):\n",
    "    \"\"\"\n",
    "    ((race_key, dr), (X, d, f, j, h, t))\n",
    "    \"\"\"\n",
    "    keys = (perform[['race_key', 'dr']])\n",
    "    X, y = get_feats(perform, numerical_cols, y_col, use_cuda)\n",
    "\n",
    "    return keys, X\n",
    "\n",
    "def computeAP(dataset, model, way='min', use_cuda=False):\n",
    "    func = min if way=='min' else max\n",
    "    ground_truth = racing_champ(dataset)\n",
    "    result, X = prep_eval_data(dataset, use_cuda)\n",
    "    score = model(*X)\n",
    "    result['win'] = score.to(device('cpu')).detach().numpy()\n",
    "    result = result.groupby(['race_key']) \\\n",
    "                .apply(lambda x: x[x['win']==func(x['win'])]) \\\n",
    "                .reset_index(drop=True)[['race_key', 'dr']]\n",
    "\n",
    "    return AveragePrecision(result, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.87s] Iter=0, train loss=0.929\n",
      "\t [VAL] AP=0.092; [TEST] AP=0.102\n",
      "[5.7s] Iter=1, train loss=0.92\n",
      "\t [VAL] AP=0.103; [TEST] AP=0.102\n",
      "[5.974s] Iter=2, train loss=0.912\n",
      "\t [VAL] AP=0.107; [TEST] AP=0.105\n",
      "[6.125s] Iter=3, train loss=0.904\n",
      "\t [VAL] AP=0.107; [TEST] AP=0.106\n",
      "[6.134s] Iter=4, train loss=0.896\n",
      "\t [VAL] AP=0.111; [TEST] AP=0.108\n",
      "[5.927s] Iter=5, train loss=0.889\n",
      "\t [VAL] AP=0.114; [TEST] AP=0.108\n",
      "[6.008s] Iter=6, train loss=0.882\n",
      "\t [VAL] AP=0.114; [TEST] AP=0.11\n",
      "[6.074s] Iter=7, train loss=0.876\n",
      "\t [VAL] AP=0.111; [TEST] AP=0.113\n",
      "[5.852s] Iter=8, train loss=0.871\n",
      "\t [VAL] AP=0.114; [TEST] AP=0.115\n",
      "[5.814s] Iter=9, train loss=0.866\n",
      "\t [VAL] AP=0.116; [TEST] AP=0.114\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor, LongTensor\n",
    "from torch import autograd, device\n",
    "from torch import optim\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegWEmb(nn.Module):\n",
    "    \"\"\" Simple Concat + Linear txfm for all embeddings \"\"\"\n",
    "    def __init__(self, n_num_feats, k_dim_field, k_dim_id) -> None:\n",
    "        super(LinearRegWEmb, self).__init__()\n",
    "        self.n_num_feats = n_num_feats\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        # init embedding for ids\n",
    "        self.emb_field = nn.Embedding(len(field2ix), k_dim_field)\n",
    "        self.emb_jockey = nn.Embedding(len(jockey2ix), k_dim_id)\n",
    "        self.emb_horse = nn.Embedding(len(horse2ix), k_dim_id)\n",
    "        self.emb_trainer = nn.Embedding(len(trainer2ix), k_dim_id)\n",
    "        # output layer\n",
    "        out_dim = n_num_feats + k_dim_field + 3*k_dim_id\n",
    "        self.Linear = nn.Linear(out_dim, 1)\n",
    "        # init all dims\n",
    "        nn.init.normal_(self.Linear.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_field.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_jockey.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_horse.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_trainer.weight, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, field, jockey, horse, trainer):\n",
    "        emb_f = self.emb_field(field)\n",
    "        emb_j = self.emb_jockey(jockey)\n",
    "        emb_h = self.emb_horse(horse)\n",
    "        emb_t = self.emb_trainer(trainer)\n",
    "\n",
    "        out = self.Linear(torch.concat([x, emb_f, emb_j, emb_h, emb_t], 1))\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "def SSE(input, target):\n",
    "    return (target-input)**2\n",
    "\n",
    "def BCELoss(input, target):\n",
    "    return nn.BCEWithLogitsLoss()(input, target)\n",
    "\n",
    "# setting\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "# model\n",
    "K_DIM_F = 4\n",
    "K_DIM_IX = 16\n",
    "# optimizer\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-3\n",
    "\n",
    "compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "model = LinearRegWEmb(n_num_feats=len(numerical_cols), k_dim_field=K_DIM_F, k_dim_id=K_DIM_IX).to(compute_device)\n",
    "opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# data prep\n",
    "X_train, y_train = get_feats(perform_train, numerical_cols, y_col, use_cuda)\n",
    "# Loss\n",
    "train_loss_by_ep = []\n",
    "# Average Percision\n",
    "val_ap_by_ep = []\n",
    "test_ap_by_ep = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "    ep_loss = []\n",
    "    for batch_data in horse_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "        x, f, j, h, t, y = batch_data\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = autograd.Variable(x)\n",
    "        f = autograd.Variable(f)\n",
    "        j = autograd.Variable(j)\n",
    "        h = autograd.Variable(h)\n",
    "        t = autograd.Variable(t)\n",
    "\n",
    "        y_pred = model(x, f, j, h, t)\n",
    "        loss = BCELoss(y_pred, y)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss.append(loss.data.to(compute_device).tolist())\n",
    "#         print(ep_loss)\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # compute AP\n",
    "    val_ap_by_ep.append(computeAP(perform_val, model, way='max', use_cuda=use_cuda))\n",
    "    test_ap_by_ep.append(computeAP(perform_test, model, way='max', use_cuda=use_cuda))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'[{round(t1-t0, 3)}s] Iter={ep}, train loss={round(train_loss_by_ep[-1], 3)}')\n",
    "    print(f'\\t [VAL] AP={round(val_ap_by_ep[-1], 3)}; [TEST] AP={round(test_ap_by_ep[-1], 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.287s] Iter=0, train loss=0.939\n",
      "\t [VAL] AP=0.094; [TEST] AP=0.084\n",
      "[6.297s] Iter=1, train loss=0.931\n",
      "\t [VAL] AP=0.096; [TEST] AP=0.085\n",
      "[6.292s] Iter=2, train loss=0.924\n",
      "\t [VAL] AP=0.092; [TEST] AP=0.086\n",
      "[6.304s] Iter=3, train loss=0.917\n",
      "\t [VAL] AP=0.089; [TEST] AP=0.087\n",
      "[6.217s] Iter=4, train loss=0.911\n",
      "\t [VAL] AP=0.097; [TEST] AP=0.087\n",
      "[6.308s] Iter=5, train loss=0.905\n",
      "\t [VAL] AP=0.103; [TEST] AP=0.087\n",
      "[6.163s] Iter=6, train loss=0.9\n",
      "\t [VAL] AP=0.099; [TEST] AP=0.086\n",
      "[6.269s] Iter=7, train loss=0.896\n",
      "\t [VAL] AP=0.095; [TEST] AP=0.087\n",
      "[6.314s] Iter=8, train loss=0.891\n",
      "\t [VAL] AP=0.093; [TEST] AP=0.087\n",
      "[6.3s] Iter=9, train loss=0.887\n",
      "\t [VAL] AP=0.088; [TEST] AP=0.088\n"
     ]
    }
   ],
   "source": [
    "class LinearRegWEmbv1(nn.Module):\n",
    "    \"\"\" Dot All Embs into 1d for scale reduction \"\"\"\n",
    "    def __init__(self, n_num_feats, k_dim_field, k_dim_id) -> None:\n",
    "        super(LinearRegWEmbv1, self).__init__()\n",
    "        self.n_num_feats = n_num_feats\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        # init embedding for ids\n",
    "        self.emb_field = nn.Embedding(len(field2ix), k_dim_field)\n",
    "        self.emb_jockey = nn.Embedding(len(jockey2ix), k_dim_id)\n",
    "        self.emb_horse = nn.Embedding(len(horse2ix), k_dim_id)\n",
    "        self.emb_trainer = nn.Embedding(len(trainer2ix), k_dim_id)\n",
    "        # output layer\n",
    "        out_dim = n_num_feats + 3\n",
    "        self.Linear = nn.Linear(out_dim, 1)\n",
    "        self.Linear_field = nn.Linear(k_dim_field, 1)\n",
    "        # init all dims\n",
    "        nn.init.normal_(self.Linear.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_field.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_jockey.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_horse.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_trainer.weight, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, field, jockey, horse, trainer):\n",
    "        emb_f = self.emb_field(field)\n",
    "        emb_j = self.emb_jockey(jockey)\n",
    "        emb_h = self.emb_horse(horse)\n",
    "        emb_t = self.emb_trainer(trainer)\n",
    "        f_val = self.Linear_field(emb_f)\n",
    "        hj_val = torch.matmul(emb_h, emb_j.T).sum(1).unsqueeze(1)\n",
    "        ht_val = torch.matmul(emb_h, emb_t.T).sum(1).unsqueeze(1)\n",
    "        \n",
    "        out = self.Linear(torch.concat([x, f_val, hj_val, ht_val],1))\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "# setting\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "# model\n",
    "K_DIM_F = 4\n",
    "K_DIM_IX = 16\n",
    "# optimizer\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-3\n",
    "use_cuda= True\n",
    "\n",
    "compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "model = LinearRegWEmbv1(n_num_feats=len(numerical_cols), k_dim_field=K_DIM_F, k_dim_id=K_DIM_IX).to(compute_device)\n",
    "opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# data prep\n",
    "X_train, y_train = get_feats(perform_train, numerical_cols, y_col, use_cuda)\n",
    "# Loss\n",
    "train_loss_by_ep = []\n",
    "# Average Percision\n",
    "val_ap_by_ep = []\n",
    "test_ap_by_ep = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "    ep_loss = []\n",
    "    for batch_data in horse_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "        x, f, j, h, t, y = batch_data\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = autograd.Variable(x)\n",
    "        f = autograd.Variable(f)\n",
    "        j = autograd.Variable(j)\n",
    "        h = autograd.Variable(h)\n",
    "        t = autograd.Variable(t)\n",
    "\n",
    "        y_pred = model(x, f, j, h, t)\n",
    "        loss = BCELoss(y_pred, y)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss.append(loss.data.to(compute_device).tolist())\n",
    "#         print(ep_loss)\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # compute AP\n",
    "    val_ap_by_ep.append(computeAP(perform_val, model, way='max', use_cuda=use_cuda))\n",
    "    test_ap_by_ep.append(computeAP(perform_test, model, way='max', use_cuda=use_cuda))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'[{round(t1-t0, 3)}s] Iter={ep}, train loss={round(train_loss_by_ep[-1], 3)}')\n",
    "    print(f'\\t [VAL] AP={round(val_ap_by_ep[-1], 3)}; [TEST] AP={round(test_ap_by_ep[-1], 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.971s] Iter=0, train loss=0.92947\n",
      "\t [VAL] AP=0.10262; [TEST] AP=0.10737\n",
      "[5.859s] Iter=1, train loss=0.92199\n",
      "\t [VAL] AP=0.09825; [TEST] AP=0.10847\n",
      "[5.885s] Iter=2, train loss=0.91505\n",
      "\t [VAL] AP=0.10262; [TEST] AP=0.11096\n",
      "[6.173s] Iter=3, train loss=0.90867\n",
      "\t [VAL] AP=0.10699; [TEST] AP=0.11179\n",
      "[6.484s] Iter=4, train loss=0.90281\n",
      "\t [VAL] AP=0.11572; [TEST] AP=0.11261\n",
      "[6.057s] Iter=5, train loss=0.8974\n",
      "\t [VAL] AP=0.12227; [TEST] AP=0.11565\n",
      "[5.993s] Iter=6, train loss=0.89241\n",
      "\t [VAL] AP=0.13537; [TEST] AP=0.11896\n",
      "[6.04s] Iter=7, train loss=0.88784\n",
      "\t [VAL] AP=0.131; [TEST] AP=0.12117\n",
      "[6.099s] Iter=8, train loss=0.88361\n",
      "\t [VAL] AP=0.131; [TEST] AP=0.122\n",
      "[5.689s] Iter=9, train loss=0.87973\n",
      "\t [VAL] AP=0.13319; [TEST] AP=0.1231\n"
     ]
    }
   ],
   "source": [
    "class LinearRegWEmbv2(nn.Module):\n",
    "    \"\"\" Element wise multiplication of embs, and linear comb \"\"\"\n",
    "    def __init__(self, n_num_feats, k_dim_field, k_dim_id) -> None:\n",
    "        super(LinearRegWEmbv2, self).__init__()\n",
    "        self.n_num_feats = n_num_feats\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        # init embedding for ids\n",
    "        self.emb_field = nn.Embedding(len(field2ix), k_dim_field)\n",
    "        self.emb_jockey = nn.Embedding(len(jockey2ix), k_dim_id)\n",
    "        self.emb_horse = nn.Embedding(len(horse2ix), k_dim_id)\n",
    "        self.emb_trainer = nn.Embedding(len(trainer2ix), k_dim_id)\n",
    "        # output layer\n",
    "        out_dim = n_num_feats + k_dim_field + 2*k_dim_id\n",
    "        self.Linear = nn.Linear(out_dim, 1)\n",
    "        # init all dims\n",
    "        nn.init.normal_(self.Linear.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_field.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_jockey.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_horse.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_trainer.weight, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, field, jockey, horse, trainer):\n",
    "        emb_f = self.emb_field(field)\n",
    "        emb_j = self.emb_jockey(jockey)\n",
    "        emb_h = self.emb_horse(horse)\n",
    "        emb_t = self.emb_trainer(trainer)\n",
    "        hj_val = torch.mul(emb_h, emb_j)\n",
    "        ht_val = torch.mul(emb_h, emb_t)\n",
    "        \n",
    "        out = self.Linear(torch.concat([x, emb_f, hj_val, ht_val], 1))\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "# setting\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "# model\n",
    "K_DIM_F = 4\n",
    "K_DIM_IX = 64\n",
    "# optimizer\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-5\n",
    "use_cuda= True\n",
    "\n",
    "compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "model = LinearRegWEmbv2(n_num_feats=len(numerical_cols), k_dim_field=K_DIM_F, k_dim_id=K_DIM_IX).to(compute_device)\n",
    "opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# data prep\n",
    "X_train, y_train = get_feats(perform_train, numerical_cols, y_col, use_cuda)\n",
    "# Loss\n",
    "train_loss_by_ep = []\n",
    "# Average Percision\n",
    "val_ap_by_ep = []\n",
    "test_ap_by_ep = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "    ep_loss = []\n",
    "    for batch_data in horse_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "        x, f, j, h, t, y = batch_data\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = autograd.Variable(x)\n",
    "        f = autograd.Variable(f)\n",
    "        j = autograd.Variable(j)\n",
    "        h = autograd.Variable(h)\n",
    "        t = autograd.Variable(t)\n",
    "\n",
    "        y_pred = model(x, f, j, h, t)\n",
    "        loss = BCELoss(y_pred, y)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss.append(loss.data.to(compute_device).tolist())\n",
    "#         print(ep_loss)\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # compute AP\n",
    "    val_ap_by_ep.append(computeAP(perform_val, model, way='max', use_cuda=use_cuda))\n",
    "    test_ap_by_ep.append(computeAP(perform_test, model, way='max', use_cuda=use_cuda))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'[{round(t1-t0, 3)}s] Iter={ep}, train loss={round(train_loss_by_ep[-1], 5)}')\n",
    "    print(f'\\t [VAL] AP={round(val_ap_by_ep[-1], 5)}; [TEST] AP={round(test_ap_by_ep[-1], 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GmfMlp(\n",
       "  (emb_field): Embedding(9, 4)\n",
       "  (emb_jockey): Embedding(131, 16)\n",
       "  (emb_horse): Embedding(4399, 16)\n",
       "  (emb_trainer): Embedding(129, 16)\n",
       "  (MLP_Layer): Sequential(\n",
       "    (0): Dropout(p=0.05, inplace=False)\n",
       "    (1): Linear(in_features=42, out_features=21, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.05, inplace=False)\n",
       "    (4): Linear(in_features=21, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.05, inplace=False)\n",
       "    (7): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (Linear): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GmfMlp(nn.Module):\n",
    "    \"\"\" Dot All Embs into 1d for scale reduction \"\"\"\n",
    "    def __init__(self, n_num_feats, k_dim_field, k_dim_id, num_layers, p_dropout=0.05) -> None:\n",
    "        super(GmfMlp, self).__init__()\n",
    "        self.n_num_feats = n_num_feats\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        # init embedding for ids\n",
    "        self.emb_field = nn.Embedding(len(field2ix), k_dim_field)\n",
    "        self.emb_jockey = nn.Embedding(len(jockey2ix), k_dim_id)\n",
    "        self.emb_horse = nn.Embedding(len(horse2ix), k_dim_id)\n",
    "        self.emb_trainer = nn.Embedding(len(trainer2ix), k_dim_id)\n",
    "        # MLP layer\n",
    "        feat_dim = n_num_feats + k_dim_field + 2*k_dim_id\n",
    "        MLP_sizes = [int(feat_dim*(0.5**i)) for i in range(num_layers+1)]\n",
    "        MLP_Layer=[]\n",
    "        for i in range(num_layers):\n",
    "            MLP_Layer.append(nn.Dropout(p_dropout))\n",
    "            MLP_Layer.append(nn.Linear(MLP_sizes[i], MLP_sizes[i+1]))\n",
    "            MLP_Layer.append(nn.ReLU())  \n",
    "        self.MLP_Layer = nn.Sequential(*MLP_Layer)\n",
    "        self.Linear = nn.Linear(MLP_sizes[-1], 1)\n",
    "        # init all dims\n",
    "        nn.init.normal_(self.Linear.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_field.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_jockey.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_horse.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_trainer.weight, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, field, jockey, horse, trainer):\n",
    "        emb_f = self.emb_field(field)\n",
    "        emb_j = self.emb_jockey(jockey)\n",
    "        emb_h = self.emb_horse(horse)\n",
    "        emb_t = self.emb_trainer(trainer)\n",
    "        hj_val = torch.mul(emb_h, emb_j)\n",
    "        ht_val = torch.mul(emb_h, emb_t)\n",
    "        \n",
    "        hidden_input = torch.concat([x, emb_f, hj_val, ht_val], 1)\n",
    "        out = self.MLP_Layer(hidden_input)\n",
    "        out = self.Linear(out)\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "model = GmfMlp(n_num_feats=6, k_dim_field=4, k_dim_id=16, num_layers=3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.134s] Iter=0, train loss=0.959\n",
      "\t [VAL] AP=0.087; [TEST] AP=0.094\n",
      "[6.2s] Iter=1, train loss=0.956\n",
      "\t [VAL] AP=0.087; [TEST] AP=0.105\n",
      "[6.224s] Iter=2, train loss=0.953\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.111\n",
      "[6.21s] Iter=3, train loss=0.95\n",
      "\t [VAL] AP=0.105; [TEST] AP=0.125\n",
      "[6.01s] Iter=4, train loss=0.946\n",
      "\t [VAL] AP=0.127; [TEST] AP=0.116\n",
      "[6.158s] Iter=5, train loss=0.942\n",
      "\t [VAL] AP=0.12; [TEST] AP=0.106\n",
      "[6.336s] Iter=6, train loss=0.937\n",
      "\t [VAL] AP=0.092; [TEST] AP=0.106\n",
      "[6.131s] Iter=7, train loss=0.933\n",
      "\t [VAL] AP=0.118; [TEST] AP=0.114\n",
      "[6.283s] Iter=8, train loss=0.927\n",
      "\t [VAL] AP=0.12; [TEST] AP=0.111\n",
      "[6.32s] Iter=9, train loss=0.922\n",
      "\t [VAL] AP=0.109; [TEST] AP=0.109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# setting\n",
    "batch_size = 25\n",
    "epochs = 10\n",
    "# model\n",
    "K_DIM_F = 4\n",
    "K_DIM_IX = 64\n",
    "num_layers = 2\n",
    "# optimizer\n",
    "learning_rate = 8e-6\n",
    "weight_decay = 1e-5\n",
    "use_cuda= True\n",
    "\n",
    "compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "model = GmfMlp(n_num_feats=len(numerical_cols), k_dim_field=K_DIM_F, k_dim_id=K_DIM_IX, num_layers=num_layers, p_dropout=0.1).to(compute_device)\n",
    "opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# data prep\n",
    "X_train, y_train = get_feats(perform_train, numerical_cols, y_col, use_cuda)\n",
    "# Loss\n",
    "train_loss_by_ep = []\n",
    "# Average Percision\n",
    "val_ap_by_ep = []\n",
    "test_ap_by_ep = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "    ep_loss = []\n",
    "    for batch_data in horse_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "        x, f, j, h, t, y = batch_data\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = autograd.Variable(x)\n",
    "        f = autograd.Variable(f)\n",
    "        j = autograd.Variable(j)\n",
    "        h = autograd.Variable(h)\n",
    "        t = autograd.Variable(t)\n",
    "\n",
    "        y_pred = model(x, f, j, h, t)\n",
    "\n",
    "        loss = BCELoss(y_pred, y)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss.append(loss.data.to(compute_device).tolist())\n",
    "#         print(ep_loss)\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # compute AP\n",
    "    val_ap_by_ep.append(computeAP(perform_val, model, way='max', use_cuda=use_cuda))\n",
    "    test_ap_by_ep.append(computeAP(perform_test, model, way='max', use_cuda=use_cuda))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'[{round(t1-t0, 3)}s] Iter={ep}, train loss={round(train_loss_by_ep[-1], 3)}')\n",
    "    print(f'\\t [VAL] AP={round(val_ap_by_ep[-1], 3)}; [TEST] AP={round(test_ap_by_ep[-1], 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Assumption 2\n",
    "\n",
    "Since all horses within a race provide us with precious information, telling us which one is faster.\n",
    "\n",
    "Hence, it is reasonable to calculate the pairwise loss with log-sigmoid loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
