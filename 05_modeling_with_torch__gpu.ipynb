{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43157, 18), (5558, 18), (5721, 18))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from horse.data.load_data import DataSet\n",
    "\n",
    "perform = DataSet()\n",
    "train, val, test = perform.train_val_test_split()\n",
    "\n",
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.87s] Iter=0, train loss=0.929\n",
      "\t [VAL] AP=0.092; [TEST] AP=0.102\n",
      "[5.7s] Iter=1, train loss=0.92\n",
      "\t [VAL] AP=0.103; [TEST] AP=0.102\n",
      "[5.974s] Iter=2, train loss=0.912\n",
      "\t [VAL] AP=0.107; [TEST] AP=0.105\n",
      "[6.125s] Iter=3, train loss=0.904\n",
      "\t [VAL] AP=0.107; [TEST] AP=0.106\n",
      "[6.134s] Iter=4, train loss=0.896\n",
      "\t [VAL] AP=0.111; [TEST] AP=0.108\n",
      "[5.927s] Iter=5, train loss=0.889\n",
      "\t [VAL] AP=0.114; [TEST] AP=0.108\n",
      "[6.008s] Iter=6, train loss=0.882\n",
      "\t [VAL] AP=0.114; [TEST] AP=0.11\n",
      "[6.074s] Iter=7, train loss=0.876\n",
      "\t [VAL] AP=0.111; [TEST] AP=0.113\n",
      "[5.852s] Iter=8, train loss=0.871\n",
      "\t [VAL] AP=0.114; [TEST] AP=0.115\n",
      "[5.814s] Iter=9, train loss=0.866\n",
      "\t [VAL] AP=0.116; [TEST] AP=0.114\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor, LongTensor\n",
    "from torch import autograd, device\n",
    "from torch import optim\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegWEmb(nn.Module):\n",
    "    \"\"\" Simple Concat + Linear txfm for all embeddings \"\"\"\n",
    "    def __init__(self, n_num_feats, k_dim_field, k_dim_id) -> None:\n",
    "        super(LinearRegWEmb, self).__init__()\n",
    "        self.n_num_feats = n_num_feats\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        # init embedding for ids\n",
    "        self.emb_field = nn.Embedding(len(field2ix), k_dim_field)\n",
    "        self.emb_jockey = nn.Embedding(len(jockey2ix), k_dim_id)\n",
    "        self.emb_horse = nn.Embedding(len(horse2ix), k_dim_id)\n",
    "        self.emb_trainer = nn.Embedding(len(trainer2ix), k_dim_id)\n",
    "        # output layer\n",
    "        out_dim = n_num_feats + k_dim_field + 3*k_dim_id\n",
    "        self.Linear = nn.Linear(out_dim, 1)\n",
    "        # init all dims\n",
    "        nn.init.normal_(self.Linear.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_field.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_jockey.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_horse.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_trainer.weight, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, field, jockey, horse, trainer):\n",
    "        emb_f = self.emb_field(field)\n",
    "        emb_j = self.emb_jockey(jockey)\n",
    "        emb_h = self.emb_horse(horse)\n",
    "        emb_t = self.emb_trainer(trainer)\n",
    "\n",
    "        out = self.Linear(torch.concat([x, emb_f, emb_j, emb_h, emb_t], 1))\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "def SSE(input, target):\n",
    "    return (target-input)**2\n",
    "\n",
    "def BCELoss(input, target):\n",
    "    return nn.BCEWithLogitsLoss()(input, target)\n",
    "\n",
    "# setting\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "# model\n",
    "K_DIM_F = 4\n",
    "K_DIM_IX = 16\n",
    "# optimizer\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-3\n",
    "\n",
    "compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "model = LinearRegWEmb(n_num_feats=len(numerical_cols), k_dim_field=K_DIM_F, k_dim_id=K_DIM_IX).to(compute_device)\n",
    "opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# data prep\n",
    "X_train, y_train = get_feats(perform_train, numerical_cols, y_col, use_cuda)\n",
    "# Loss\n",
    "train_loss_by_ep = []\n",
    "# Average Percision\n",
    "val_ap_by_ep = []\n",
    "test_ap_by_ep = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "    ep_loss = []\n",
    "    for batch_data in horse_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "        x, f, j, h, t, y = batch_data\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = autograd.Variable(x)\n",
    "        f = autograd.Variable(f)\n",
    "        j = autograd.Variable(j)\n",
    "        h = autograd.Variable(h)\n",
    "        t = autograd.Variable(t)\n",
    "\n",
    "        y_pred = model(x, f, j, h, t)\n",
    "        loss = BCELoss(y_pred, y)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss.append(loss.data.to(compute_device).tolist())\n",
    "#         print(ep_loss)\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # compute AP\n",
    "    val_ap_by_ep.append(computeAP(perform_val, model, way='max', use_cuda=use_cuda))\n",
    "    test_ap_by_ep.append(computeAP(perform_test, model, way='max', use_cuda=use_cuda))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'[{round(t1-t0, 3)}s] Iter={ep}, train loss={round(train_loss_by_ep[-1], 3)}')\n",
    "    print(f'\\t [VAL] AP={round(val_ap_by_ep[-1], 3)}; [TEST] AP={round(test_ap_by_ep[-1], 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.287s] Iter=0, train loss=0.939\n",
      "\t [VAL] AP=0.094; [TEST] AP=0.084\n",
      "[6.297s] Iter=1, train loss=0.931\n",
      "\t [VAL] AP=0.096; [TEST] AP=0.085\n",
      "[6.292s] Iter=2, train loss=0.924\n",
      "\t [VAL] AP=0.092; [TEST] AP=0.086\n",
      "[6.304s] Iter=3, train loss=0.917\n",
      "\t [VAL] AP=0.089; [TEST] AP=0.087\n",
      "[6.217s] Iter=4, train loss=0.911\n",
      "\t [VAL] AP=0.097; [TEST] AP=0.087\n",
      "[6.308s] Iter=5, train loss=0.905\n",
      "\t [VAL] AP=0.103; [TEST] AP=0.087\n",
      "[6.163s] Iter=6, train loss=0.9\n",
      "\t [VAL] AP=0.099; [TEST] AP=0.086\n",
      "[6.269s] Iter=7, train loss=0.896\n",
      "\t [VAL] AP=0.095; [TEST] AP=0.087\n",
      "[6.314s] Iter=8, train loss=0.891\n",
      "\t [VAL] AP=0.093; [TEST] AP=0.087\n",
      "[6.3s] Iter=9, train loss=0.887\n",
      "\t [VAL] AP=0.088; [TEST] AP=0.088\n"
     ]
    }
   ],
   "source": [
    "class LinearRegWEmbv1(nn.Module):\n",
    "    \"\"\" Dot All Embs into 1d for scale reduction \"\"\"\n",
    "    def __init__(self, n_num_feats, k_dim_field, k_dim_id) -> None:\n",
    "        super(LinearRegWEmbv1, self).__init__()\n",
    "        self.n_num_feats = n_num_feats\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        # init embedding for ids\n",
    "        self.emb_field = nn.Embedding(len(field2ix), k_dim_field)\n",
    "        self.emb_jockey = nn.Embedding(len(jockey2ix), k_dim_id)\n",
    "        self.emb_horse = nn.Embedding(len(horse2ix), k_dim_id)\n",
    "        self.emb_trainer = nn.Embedding(len(trainer2ix), k_dim_id)\n",
    "        # output layer\n",
    "        out_dim = n_num_feats + 3\n",
    "        self.Linear = nn.Linear(out_dim, 1)\n",
    "        self.Linear_field = nn.Linear(k_dim_field, 1)\n",
    "        # init all dims\n",
    "        nn.init.normal_(self.Linear.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_field.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_jockey.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_horse.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_trainer.weight, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, field, jockey, horse, trainer):\n",
    "        emb_f = self.emb_field(field)\n",
    "        emb_j = self.emb_jockey(jockey)\n",
    "        emb_h = self.emb_horse(horse)\n",
    "        emb_t = self.emb_trainer(trainer)\n",
    "        f_val = self.Linear_field(emb_f)\n",
    "        hj_val = torch.matmul(emb_h, emb_j.T).sum(1).unsqueeze(1)\n",
    "        ht_val = torch.matmul(emb_h, emb_t.T).sum(1).unsqueeze(1)\n",
    "        \n",
    "        out = self.Linear(torch.concat([x, f_val, hj_val, ht_val],1))\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "# setting\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "# model\n",
    "K_DIM_F = 4\n",
    "K_DIM_IX = 16\n",
    "# optimizer\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-3\n",
    "use_cuda= True\n",
    "\n",
    "compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "model = LinearRegWEmbv1(n_num_feats=len(numerical_cols), k_dim_field=K_DIM_F, k_dim_id=K_DIM_IX).to(compute_device)\n",
    "opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# data prep\n",
    "X_train, y_train = get_feats(perform_train, numerical_cols, y_col, use_cuda)\n",
    "# Loss\n",
    "train_loss_by_ep = []\n",
    "# Average Percision\n",
    "val_ap_by_ep = []\n",
    "test_ap_by_ep = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "    ep_loss = []\n",
    "    for batch_data in horse_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "        x, f, j, h, t, y = batch_data\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = autograd.Variable(x)\n",
    "        f = autograd.Variable(f)\n",
    "        j = autograd.Variable(j)\n",
    "        h = autograd.Variable(h)\n",
    "        t = autograd.Variable(t)\n",
    "\n",
    "        y_pred = model(x, f, j, h, t)\n",
    "        loss = BCELoss(y_pred, y)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss.append(loss.data.to(compute_device).tolist())\n",
    "#         print(ep_loss)\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # compute AP\n",
    "    val_ap_by_ep.append(computeAP(perform_val, model, way='max', use_cuda=use_cuda))\n",
    "    test_ap_by_ep.append(computeAP(perform_test, model, way='max', use_cuda=use_cuda))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'[{round(t1-t0, 3)}s] Iter={ep}, train loss={round(train_loss_by_ep[-1], 3)}')\n",
    "    print(f'\\t [VAL] AP={round(val_ap_by_ep[-1], 3)}; [TEST] AP={round(test_ap_by_ep[-1], 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.971s] Iter=0, train loss=0.92947\n",
      "\t [VAL] AP=0.10262; [TEST] AP=0.10737\n",
      "[5.859s] Iter=1, train loss=0.92199\n",
      "\t [VAL] AP=0.09825; [TEST] AP=0.10847\n",
      "[5.885s] Iter=2, train loss=0.91505\n",
      "\t [VAL] AP=0.10262; [TEST] AP=0.11096\n",
      "[6.173s] Iter=3, train loss=0.90867\n",
      "\t [VAL] AP=0.10699; [TEST] AP=0.11179\n",
      "[6.484s] Iter=4, train loss=0.90281\n",
      "\t [VAL] AP=0.11572; [TEST] AP=0.11261\n",
      "[6.057s] Iter=5, train loss=0.8974\n",
      "\t [VAL] AP=0.12227; [TEST] AP=0.11565\n",
      "[5.993s] Iter=6, train loss=0.89241\n",
      "\t [VAL] AP=0.13537; [TEST] AP=0.11896\n",
      "[6.04s] Iter=7, train loss=0.88784\n",
      "\t [VAL] AP=0.131; [TEST] AP=0.12117\n",
      "[6.099s] Iter=8, train loss=0.88361\n",
      "\t [VAL] AP=0.131; [TEST] AP=0.122\n",
      "[5.689s] Iter=9, train loss=0.87973\n",
      "\t [VAL] AP=0.13319; [TEST] AP=0.1231\n"
     ]
    }
   ],
   "source": [
    "class LinearRegWEmbv2(nn.Module):\n",
    "    \"\"\" Element wise multiplication of embs, and linear comb \"\"\"\n",
    "    def __init__(self, n_num_feats, k_dim_field, k_dim_id) -> None:\n",
    "        super(LinearRegWEmbv2, self).__init__()\n",
    "        self.n_num_feats = n_num_feats\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        # init embedding for ids\n",
    "        self.emb_field = nn.Embedding(len(field2ix), k_dim_field)\n",
    "        self.emb_jockey = nn.Embedding(len(jockey2ix), k_dim_id)\n",
    "        self.emb_horse = nn.Embedding(len(horse2ix), k_dim_id)\n",
    "        self.emb_trainer = nn.Embedding(len(trainer2ix), k_dim_id)\n",
    "        # output layer\n",
    "        out_dim = n_num_feats + k_dim_field + 2*k_dim_id\n",
    "        self.Linear = nn.Linear(out_dim, 1)\n",
    "        # init all dims\n",
    "        nn.init.normal_(self.Linear.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_field.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_jockey.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_horse.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_trainer.weight, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, field, jockey, horse, trainer):\n",
    "        emb_f = self.emb_field(field)\n",
    "        emb_j = self.emb_jockey(jockey)\n",
    "        emb_h = self.emb_horse(horse)\n",
    "        emb_t = self.emb_trainer(trainer)\n",
    "        hj_val = torch.mul(emb_h, emb_j)\n",
    "        ht_val = torch.mul(emb_h, emb_t)\n",
    "        \n",
    "        out = self.Linear(torch.concat([x, emb_f, hj_val, ht_val], 1))\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "# setting\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "# model\n",
    "K_DIM_F = 4\n",
    "K_DIM_IX = 64\n",
    "# optimizer\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 1e-5\n",
    "use_cuda= True\n",
    "\n",
    "compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "model = LinearRegWEmbv2(n_num_feats=len(numerical_cols), k_dim_field=K_DIM_F, k_dim_id=K_DIM_IX).to(compute_device)\n",
    "opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# data prep\n",
    "X_train, y_train = get_feats(perform_train, numerical_cols, y_col, use_cuda)\n",
    "# Loss\n",
    "train_loss_by_ep = []\n",
    "# Average Percision\n",
    "val_ap_by_ep = []\n",
    "test_ap_by_ep = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "    ep_loss = []\n",
    "    for batch_data in horse_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "        x, f, j, h, t, y = batch_data\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = autograd.Variable(x)\n",
    "        f = autograd.Variable(f)\n",
    "        j = autograd.Variable(j)\n",
    "        h = autograd.Variable(h)\n",
    "        t = autograd.Variable(t)\n",
    "\n",
    "        y_pred = model(x, f, j, h, t)\n",
    "        loss = BCELoss(y_pred, y)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss.append(loss.data.to(compute_device).tolist())\n",
    "#         print(ep_loss)\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # compute AP\n",
    "    val_ap_by_ep.append(computeAP(perform_val, model, way='max', use_cuda=use_cuda))\n",
    "    test_ap_by_ep.append(computeAP(perform_test, model, way='max', use_cuda=use_cuda))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'[{round(t1-t0, 3)}s] Iter={ep}, train loss={round(train_loss_by_ep[-1], 5)}')\n",
    "    print(f'\\t [VAL] AP={round(val_ap_by_ep[-1], 5)}; [TEST] AP={round(test_ap_by_ep[-1], 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GmfMlp(\n",
       "  (emb_field): Embedding(9, 4)\n",
       "  (emb_jockey): Embedding(131, 16)\n",
       "  (emb_horse): Embedding(4399, 16)\n",
       "  (emb_trainer): Embedding(129, 16)\n",
       "  (MLP_Layer): Sequential(\n",
       "    (0): Dropout(p=0.05, inplace=False)\n",
       "    (1): Linear(in_features=42, out_features=21, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.05, inplace=False)\n",
       "    (4): Linear(in_features=21, out_features=10, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.05, inplace=False)\n",
       "    (7): Linear(in_features=10, out_features=5, bias=True)\n",
       "    (8): ReLU()\n",
       "  )\n",
       "  (Linear): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GmfMlp(nn.Module):\n",
    "    \"\"\" Dot All Embs into 1d for scale reduction \"\"\"\n",
    "    def __init__(self, n_num_feats, k_dim_field, k_dim_id, num_layers, p_dropout=0.05) -> None:\n",
    "        super(GmfMlp, self).__init__()\n",
    "        self.n_num_feats = n_num_feats\n",
    "        self.sigmoid = torch.sigmoid\n",
    "        # init embedding for ids\n",
    "        self.emb_field = nn.Embedding(len(field2ix), k_dim_field)\n",
    "        self.emb_jockey = nn.Embedding(len(jockey2ix), k_dim_id)\n",
    "        self.emb_horse = nn.Embedding(len(horse2ix), k_dim_id)\n",
    "        self.emb_trainer = nn.Embedding(len(trainer2ix), k_dim_id)\n",
    "        # MLP layer\n",
    "        feat_dim = n_num_feats + k_dim_field + 2*k_dim_id\n",
    "        MLP_sizes = [int(feat_dim*(0.5**i)) for i in range(num_layers+1)]\n",
    "        MLP_Layer=[]\n",
    "        for i in range(num_layers):\n",
    "            MLP_Layer.append(nn.Dropout(p_dropout))\n",
    "            MLP_Layer.append(nn.Linear(MLP_sizes[i], MLP_sizes[i+1]))\n",
    "            MLP_Layer.append(nn.ReLU())  \n",
    "        self.MLP_Layer = nn.Sequential(*MLP_Layer)\n",
    "        self.Linear = nn.Linear(MLP_sizes[-1], 1)\n",
    "        # init all dims\n",
    "        nn.init.normal_(self.Linear.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_field.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_jockey.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_horse.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.emb_trainer.weight, mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x, field, jockey, horse, trainer):\n",
    "        emb_f = self.emb_field(field)\n",
    "        emb_j = self.emb_jockey(jockey)\n",
    "        emb_h = self.emb_horse(horse)\n",
    "        emb_t = self.emb_trainer(trainer)\n",
    "        hj_val = torch.mul(emb_h, emb_j)\n",
    "        ht_val = torch.mul(emb_h, emb_t)\n",
    "        \n",
    "        hidden_input = torch.concat([x, emb_f, hj_val, ht_val], 1)\n",
    "        out = self.MLP_Layer(hidden_input)\n",
    "        out = self.Linear(out)\n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "model = GmfMlp(n_num_feats=6, k_dim_field=4, k_dim_id=16, num_layers=3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.134s] Iter=0, train loss=0.959\n",
      "\t [VAL] AP=0.087; [TEST] AP=0.094\n",
      "[6.2s] Iter=1, train loss=0.956\n",
      "\t [VAL] AP=0.087; [TEST] AP=0.105\n",
      "[6.224s] Iter=2, train loss=0.953\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.111\n",
      "[6.21s] Iter=3, train loss=0.95\n",
      "\t [VAL] AP=0.105; [TEST] AP=0.125\n",
      "[6.01s] Iter=4, train loss=0.946\n",
      "\t [VAL] AP=0.127; [TEST] AP=0.116\n",
      "[6.158s] Iter=5, train loss=0.942\n",
      "\t [VAL] AP=0.12; [TEST] AP=0.106\n",
      "[6.336s] Iter=6, train loss=0.937\n",
      "\t [VAL] AP=0.092; [TEST] AP=0.106\n",
      "[6.131s] Iter=7, train loss=0.933\n",
      "\t [VAL] AP=0.118; [TEST] AP=0.114\n",
      "[6.283s] Iter=8, train loss=0.927\n",
      "\t [VAL] AP=0.12; [TEST] AP=0.111\n",
      "[6.32s] Iter=9, train loss=0.922\n",
      "\t [VAL] AP=0.109; [TEST] AP=0.109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# setting\n",
    "batch_size = 25\n",
    "epochs = 10\n",
    "# model\n",
    "K_DIM_F = 4\n",
    "K_DIM_IX = 64\n",
    "num_layers = 2\n",
    "# optimizer\n",
    "learning_rate = 8e-6\n",
    "weight_decay = 1e-5\n",
    "use_cuda= True\n",
    "\n",
    "compute_device = device('cuda') if use_cuda else device('cpu')\n",
    "\n",
    "model = GmfMlp(n_num_feats=len(numerical_cols), k_dim_field=K_DIM_F, k_dim_id=K_DIM_IX, num_layers=num_layers, p_dropout=0.1).to(compute_device)\n",
    "opt = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# data prep\n",
    "X_train, y_train = get_feats(perform_train, numerical_cols, y_col, use_cuda)\n",
    "# Loss\n",
    "train_loss_by_ep = []\n",
    "# Average Percision\n",
    "val_ap_by_ep = []\n",
    "test_ap_by_ep = []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    t0 = time.time()\n",
    "    ep_loss = []\n",
    "    for batch_data in horse_data_loader(X_train, y_train, batch_size, shuffle=True):\n",
    "        x, f, j, h, t, y = batch_data\n",
    "        model.zero_grad()\n",
    "\n",
    "        x = autograd.Variable(x)\n",
    "        f = autograd.Variable(f)\n",
    "        j = autograd.Variable(j)\n",
    "        h = autograd.Variable(h)\n",
    "        t = autograd.Variable(t)\n",
    "\n",
    "        y_pred = model(x, f, j, h, t)\n",
    "\n",
    "        loss = BCELoss(y_pred, y)\n",
    "        loss.mean().backward()\n",
    "        opt.step()\n",
    "\n",
    "        ep_loss.append(loss.data.to(compute_device).tolist())\n",
    "#         print(ep_loss)\n",
    "\n",
    "    train_loss_by_ep.append(np.sqrt(np.mean(ep_loss)))\n",
    "    \n",
    "    # compute AP\n",
    "    val_ap_by_ep.append(computeAP(perform_val, model, way='max', use_cuda=use_cuda))\n",
    "    test_ap_by_ep.append(computeAP(perform_test, model, way='max', use_cuda=use_cuda))\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f'[{round(t1-t0, 3)}s] Iter={ep}, train loss={round(train_loss_by_ep[-1], 3)}')\n",
    "    print(f'\\t [VAL] AP={round(val_ap_by_ep[-1], 3)}; [TEST] AP={round(test_ap_by_ep[-1], 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Assumption 2\n",
    "\n",
    "Since all horses within a race provide us with precious information, telling us which one is faster.\n",
    "\n",
    "Hence, it is reasonable to calculate the pairwise loss with log-sigmoid loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
