{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=32, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbConcat', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=True, use_cuda=True, use_numeric=False, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "['race_key', 'race_date', 'dr', 'dr_ix', 'field_going', 'jockey', 'horse', 'trainer', 'race_money', 'horse_bestperform_h', 'horse_champ_rate_h', 'horse_champs_h', 'horse_life_time_d', 'horse_place_m', 'horse_top4_rate_y', 'horse_top4_h', 'horse_top4_last5', 'jockey_champ_rate_h', 'jockey_champ_last5', 'jockey_champ_y', 'jockey_champ_rate_y', 'jockey_elo', 'jockey_top4_rate_y', 'jockey_top4_m', 'jockey_top4_rate_m', 'trainer_champ_rate_h', 'trainer_champ_rate_y', 'trainer_place_rate_h', 'trainer_top4_rate_y', 'trainer_top4_rate_m', 'is_champ', 'pla']\n",
      "MODEL: LinEmbConcat(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 32)\n",
      "  (emb_horse): Embedding(4399, 32)\n",
      "  (emb_trainer): Embedding(129, 32)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=104, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[12.011s] Iter=0, train loss=0.695\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.083\n",
      "[11.316s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.076; [TEST] AP=0.088\n",
      "[10.989s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.097; [TEST] AP=0.083\n",
      "[11.341s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.081; [TEST] AP=0.067\n",
      "[11.48s] Iter=4, train loss=0.693\n",
      "\t [VAL] AP=0.078; [TEST] AP=0.073\n",
      "[11.53s] Iter=5, train loss=0.693\n",
      "\t [VAL] AP=0.133; [TEST] AP=0.1\n",
      "[11.313s] Iter=6, train loss=0.693\n",
      "\t [VAL] AP=0.123; [TEST] AP=0.104\n",
      "[11.365s] Iter=7, train loss=0.693\n",
      "\t [VAL] AP=0.148; [TEST] AP=0.123\n",
      "[11.527s] Iter=8, train loss=0.693\n",
      "\t [VAL] AP=0.169; [TEST] AP=0.123\n",
      "[11.46s] Iter=9, train loss=0.693\n",
      "\t [VAL] AP=0.172; [TEST] AP=0.148\n",
      "[11.54s] Iter=10, train loss=0.693\n",
      "\t [VAL] AP=0.176; [TEST] AP=0.154\n",
      "[11.244s] Iter=11, train loss=0.693\n",
      "\t [VAL] AP=0.172; [TEST] AP=0.14\n",
      "[11.478s] Iter=12, train loss=0.692\n",
      "\t [VAL] AP=0.191; [TEST] AP=0.142\n",
      "[11.496s] Iter=13, train loss=0.692\n",
      "\t [VAL] AP=0.186; [TEST] AP=0.146\n",
      "[11.449s] Iter=14, train loss=0.692\n",
      "\t [VAL] AP=0.191; [TEST] AP=0.148\n",
      "[11.446s] Iter=15, train loss=0.692\n",
      "\t [VAL] AP=0.191; [TEST] AP=0.142\n",
      "[11.356s] Iter=16, train loss=0.692\n",
      "\t [VAL] AP=0.189; [TEST] AP=0.123\n",
      "[11.499s] Iter=17, train loss=0.692\n",
      "\t [VAL] AP=0.195; [TEST] AP=0.142\n",
      "[11.212s] Iter=18, train loss=0.692\n",
      "\t [VAL] AP=0.184; [TEST] AP=0.14\n",
      "[11.529s] Iter=19, train loss=0.692\n",
      "\t [VAL] AP=0.184; [TEST] AP=0.133\n",
      "[11.351s] Iter=20, train loss=0.692\n",
      "\t [VAL] AP=0.191; [TEST] AP=0.131\n",
      "[11.451s] Iter=21, train loss=0.692\n",
      "\t [VAL] AP=0.191; [TEST] AP=0.148\n",
      "[11.697s] Iter=22, train loss=0.692\n",
      "\t [VAL] AP=0.189; [TEST] AP=0.14\n",
      "[11.395s] Iter=23, train loss=0.692\n",
      "\t [VAL] AP=0.186; [TEST] AP=0.148\n",
      "[11.417s] Iter=24, train loss=0.692\n",
      "\t [VAL] AP=0.189; [TEST] AP=0.138\n",
      "[11.514s] Iter=25, train loss=0.692\n",
      "\t [VAL] AP=0.18; [TEST] AP=0.148\n",
      "[11.306s] Iter=26, train loss=0.692\n",
      "\t [VAL] AP=0.195; [TEST] AP=0.138\n",
      "[11.267s] Iter=27, train loss=0.692\n",
      "\t [VAL] AP=0.206; [TEST] AP=0.133\n",
      "[11.487s] Iter=28, train loss=0.692\n",
      "\t [VAL] AP=0.196; [TEST] AP=0.147\n",
      "[11.48s] Iter=29, train loss=0.691\n",
      "\t [VAL] AP=0.2; [TEST] AP=0.141\n",
      "[11.314s] Iter=30, train loss=0.691\n",
      "\t [VAL] AP=0.211; [TEST] AP=0.142\n",
      "[11.522s] Iter=31, train loss=0.691\n",
      "\t [VAL] AP=0.199; [TEST] AP=0.139\n",
      "[11.544s] Iter=32, train loss=0.691\n",
      "\t [VAL] AP=0.203; [TEST] AP=0.139\n",
      "[11.444s] Iter=33, train loss=0.691\n",
      "\t [VAL] AP=0.211; [TEST] AP=0.143\n",
      "[11.217s] Iter=34, train loss=0.691\n",
      "\t [VAL] AP=0.194; [TEST] AP=0.134\n",
      "[11.281s] Iter=35, train loss=0.691\n",
      "\t [VAL] AP=0.195; [TEST] AP=0.139\n",
      "[11.192s] Iter=36, train loss=0.691\n",
      "\t [VAL] AP=0.192; [TEST] AP=0.133\n",
      "[11.375s] Iter=37, train loss=0.691\n",
      "\t [VAL] AP=0.188; [TEST] AP=0.143\n",
      "[11.459s] Iter=38, train loss=0.691\n",
      "\t [VAL] AP=0.197; [TEST] AP=0.135\n",
      "[11.252s] Iter=39, train loss=0.691\n",
      "\t [VAL] AP=0.196; [TEST] AP=0.128\n",
      "[11.237s] Iter=40, train loss=0.691\n",
      "\t [VAL] AP=0.2; [TEST] AP=0.131\n",
      "[11.082s] Iter=41, train loss=0.691\n",
      "\t [VAL] AP=0.203; [TEST] AP=0.129\n",
      "[11.416s] Iter=42, train loss=0.691\n",
      "\t [VAL] AP=0.2; [TEST] AP=0.147\n",
      "[11.32s] Iter=43, train loss=0.691\n",
      "\t [VAL] AP=0.2; [TEST] AP=0.135\n",
      "[11.342s] Iter=44, train loss=0.691\n",
      "\t [VAL] AP=0.196; [TEST] AP=0.132\n",
      "[11.407s] Iter=45, train loss=0.691\n",
      "\t [VAL] AP=0.198; [TEST] AP=0.133\n",
      "[11.285s] Iter=46, train loss=0.691\n",
      "\t [VAL] AP=0.189; [TEST] AP=0.126\n",
      "[11.466s] Iter=47, train loss=0.691\n",
      "\t [VAL] AP=0.185; [TEST] AP=0.124\n",
      "[11.457s] Iter=48, train loss=0.691\n",
      "\t [VAL] AP=0.192; [TEST] AP=0.133\n",
      "[11.456s] Iter=49, train loss=0.691\n",
      "\t [VAL] AP=0.192; [TEST] AP=0.124\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbConcat --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=32, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbConcat', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=False, use_cuda=True, use_numeric=True, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 109, 'k_dim_field': 4, 'k_dim_id': 32, 'need_activation': True}\n",
      "MODEL: LinEmbConcat(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 32)\n",
      "  (emb_horse): Embedding(4399, 32)\n",
      "  (emb_trainer): Embedding(129, 32)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=213, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[12.083s] Iter=0, train loss=0.694\n",
      "\t [VAL] AP=0.049; [TEST] AP=0.052\n",
      "[11.877s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.049; [TEST] AP=0.05\n",
      "[11.427s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.047; [TEST] AP=0.052\n",
      "[11.491s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.044; [TEST] AP=0.05\n",
      "[11.353s] Iter=4, train loss=0.693\n",
      "\t [VAL] AP=0.064; [TEST] AP=0.054\n",
      "[11.559s] Iter=5, train loss=0.693\n",
      "\t [VAL] AP=0.051; [TEST] AP=0.056\n",
      "[11.634s] Iter=6, train loss=0.693\n",
      "\t [VAL] AP=0.142; [TEST] AP=0.119\n",
      "[11.664s] Iter=7, train loss=0.693\n",
      "\t [VAL] AP=0.1; [TEST] AP=0.083\n",
      "[11.134s] Iter=8, train loss=0.693\n",
      "\t [VAL] AP=0.144; [TEST] AP=0.127\n",
      "[11.575s] Iter=9, train loss=0.693\n",
      "\t [VAL] AP=0.144; [TEST] AP=0.11\n",
      "[11.686s] Iter=10, train loss=0.692\n",
      "\t [VAL] AP=0.136; [TEST] AP=0.102\n",
      "[11.719s] Iter=11, train loss=0.692\n",
      "\t [VAL] AP=0.15; [TEST] AP=0.106\n",
      "[11.623s] Iter=12, train loss=0.692\n",
      "\t [VAL] AP=0.174; [TEST] AP=0.119\n",
      "[11.644s] Iter=13, train loss=0.692\n",
      "\t [VAL] AP=0.159; [TEST] AP=0.102\n",
      "[11.695s] Iter=14, train loss=0.692\n",
      "\t [VAL] AP=0.167; [TEST] AP=0.1\n",
      "[11.43s] Iter=15, train loss=0.692\n",
      "\t [VAL] AP=0.184; [TEST] AP=0.125\n",
      "[11.553s] Iter=16, train loss=0.692\n",
      "\t [VAL] AP=0.157; [TEST] AP=0.098\n",
      "[11.693s] Iter=17, train loss=0.691\n",
      "\t [VAL] AP=0.184; [TEST] AP=0.096\n",
      "[11.396s] Iter=18, train loss=0.691\n",
      "\t [VAL] AP=0.163; [TEST] AP=0.092\n",
      "[11.703s] Iter=19, train loss=0.691\n",
      "\t [VAL] AP=0.188; [TEST] AP=0.129\n",
      "[11.502s] Iter=20, train loss=0.691\n",
      "\t [VAL] AP=0.202; [TEST] AP=0.114\n",
      "[11.35s] Iter=21, train loss=0.691\n",
      "\t [VAL] AP=0.184; [TEST] AP=0.092\n",
      "[11.509s] Iter=22, train loss=0.691\n",
      "\t [VAL] AP=0.189; [TEST] AP=0.109\n",
      "[11.562s] Iter=23, train loss=0.691\n",
      "\t [VAL] AP=0.2; [TEST] AP=0.121\n",
      "[11.259s] Iter=24, train loss=0.691\n",
      "\t [VAL] AP=0.188; [TEST] AP=0.079\n",
      "[11.488s] Iter=25, train loss=0.691\n",
      "\t [VAL] AP=0.196; [TEST] AP=0.118\n",
      "[11.641s] Iter=26, train loss=0.691\n",
      "\t [VAL] AP=0.192; [TEST] AP=0.13\n",
      "[11.512s] Iter=27, train loss=0.691\n",
      "\t [VAL] AP=0.177; [TEST] AP=0.117\n",
      "[11.48s] Iter=28, train loss=0.691\n",
      "\t [VAL] AP=0.176; [TEST] AP=0.104\n",
      "[11.481s] Iter=29, train loss=0.691\n",
      "\t [VAL] AP=0.173; [TEST] AP=0.101\n",
      "[11.457s] Iter=30, train loss=0.691\n",
      "\t [VAL] AP=0.187; [TEST] AP=0.11\n",
      "[11.627s] Iter=31, train loss=0.691\n",
      "\t [VAL] AP=0.188; [TEST] AP=0.101\n",
      "[11.449s] Iter=32, train loss=0.691\n",
      "\t [VAL] AP=0.182; [TEST] AP=0.122\n",
      "[11.734s] Iter=33, train loss=0.691\n",
      "\t [VAL] AP=0.179; [TEST] AP=0.107\n",
      "[11.631s] Iter=34, train loss=0.691\n",
      "\t [VAL] AP=0.182; [TEST] AP=0.115\n",
      "[11.673s] Iter=35, train loss=0.691\n",
      "\t [VAL] AP=0.169; [TEST] AP=0.129\n",
      "[11.146s] Iter=36, train loss=0.691\n",
      "\t [VAL] AP=0.192; [TEST] AP=0.137\n",
      "[11.473s] Iter=37, train loss=0.69\n",
      "\t [VAL] AP=0.17; [TEST] AP=0.118\n",
      "[11.626s] Iter=38, train loss=0.691\n",
      "\t [VAL] AP=0.165; [TEST] AP=0.111\n",
      "[11.648s] Iter=39, train loss=0.69\n",
      "\t [VAL] AP=0.191; [TEST] AP=0.127\n",
      "[11.646s] Iter=40, train loss=0.69\n",
      "\t [VAL] AP=0.177; [TEST] AP=0.127\n",
      "[11.609s] Iter=41, train loss=0.69\n",
      "\t [VAL] AP=0.189; [TEST] AP=0.124\n",
      "[11.466s] Iter=42, train loss=0.69\n",
      "\t [VAL] AP=0.185; [TEST] AP=0.131\n",
      "[11.54s] Iter=43, train loss=0.69\n",
      "\t [VAL] AP=0.192; [TEST] AP=0.135\n",
      "[11.662s] Iter=44, train loss=0.69\n",
      "\t [VAL] AP=0.159; [TEST] AP=0.116\n",
      "[11.404s] Iter=45, train loss=0.69\n",
      "\t [VAL] AP=0.16; [TEST] AP=0.113\n",
      "[11.432s] Iter=46, train loss=0.69\n",
      "\t [VAL] AP=0.196; [TEST] AP=0.126\n",
      "[11.387s] Iter=47, train loss=0.69\n",
      "\t [VAL] AP=0.161; [TEST] AP=0.116\n",
      "[11.5s] Iter=48, train loss=0.69\n",
      "\t [VAL] AP=0.165; [TEST] AP=0.123\n",
      "[11.056s] Iter=49, train loss=0.69\n",
      "\t [VAL] AP=0.167; [TEST] AP=0.118\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbConcat --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001 --use_numeric True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=32, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbConcat', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=True, use_cuda=True, use_numeric=True, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 22, 'k_dim_field': 4, 'k_dim_id': 32, 'need_activation': True}\n",
      "MODEL: LinEmbConcat(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 32)\n",
      "  (emb_horse): Embedding(4399, 32)\n",
      "  (emb_trainer): Embedding(129, 32)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=126, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[12.426s] Iter=0, train loss=0.694\n",
      "\t [VAL] AP=0.061; [TEST] AP=0.065\n",
      "[11.511s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.066; [TEST] AP=0.069\n",
      "[11.26s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.064; [TEST] AP=0.062\n",
      "[11.158s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.07; [TEST] AP=0.062\n",
      "[11.417s] Iter=4, train loss=0.693\n",
      "\t [VAL] AP=0.127; [TEST] AP=0.156\n",
      "[11.635s] Iter=5, train loss=0.693\n",
      "\t [VAL] AP=0.095; [TEST] AP=0.129\n",
      "[11.284s] Iter=6, train loss=0.693\n",
      "\t [VAL] AP=0.108; [TEST] AP=0.123\n",
      "[11.314s] Iter=7, train loss=0.693\n",
      "\t [VAL] AP=0.102; [TEST] AP=0.115\n",
      "[11.445s] Iter=8, train loss=0.693\n",
      "\t [VAL] AP=0.133; [TEST] AP=0.138\n",
      "[11.406s] Iter=9, train loss=0.693\n",
      "\t [VAL] AP=0.144; [TEST] AP=0.148\n",
      "[11.244s] Iter=10, train loss=0.693\n",
      "\t [VAL] AP=0.163; [TEST] AP=0.148\n",
      "[11.231s] Iter=11, train loss=0.692\n",
      "\t [VAL] AP=0.144; [TEST] AP=0.129\n",
      "[11.54s] Iter=12, train loss=0.692\n",
      "\t [VAL] AP=0.167; [TEST] AP=0.119\n",
      "[11.41s] Iter=13, train loss=0.692\n",
      "\t [VAL] AP=0.189; [TEST] AP=0.142\n",
      "[11.692s] Iter=14, train loss=0.691\n",
      "\t [VAL] AP=0.201; [TEST] AP=0.158\n",
      "[10.574s] Iter=15, train loss=0.691\n",
      "\t [VAL] AP=0.182; [TEST] AP=0.135\n",
      "[10.761s] Iter=16, train loss=0.69\n",
      "\t [VAL] AP=0.167; [TEST] AP=0.11\n",
      "[10.972s] Iter=17, train loss=0.69\n",
      "\t [VAL] AP=0.169; [TEST] AP=0.117\n",
      "[10.701s] Iter=18, train loss=0.69\n",
      "\t [VAL] AP=0.159; [TEST] AP=0.135\n",
      "[10.605s] Iter=19, train loss=0.69\n",
      "\t [VAL] AP=0.176; [TEST] AP=0.125\n",
      "[10.431s] Iter=20, train loss=0.69\n",
      "\t [VAL] AP=0.161; [TEST] AP=0.121\n",
      "[10.432s] Iter=21, train loss=0.69\n",
      "\t [VAL] AP=0.165; [TEST] AP=0.121\n",
      "[10.673s] Iter=22, train loss=0.69\n",
      "\t [VAL] AP=0.182; [TEST] AP=0.139\n",
      "[10.883s] Iter=23, train loss=0.69\n",
      "\t [VAL] AP=0.169; [TEST] AP=0.127\n",
      "[10.728s] Iter=24, train loss=0.69\n",
      "\t [VAL] AP=0.167; [TEST] AP=0.127\n",
      "[10.732s] Iter=25, train loss=0.69\n",
      "\t [VAL] AP=0.171; [TEST] AP=0.127\n",
      "[10.77s] Iter=26, train loss=0.69\n",
      "\t [VAL] AP=0.168; [TEST] AP=0.129\n",
      "[10.723s] Iter=27, train loss=0.69\n",
      "\t [VAL] AP=0.163; [TEST] AP=0.108\n",
      "[10.43s] Iter=28, train loss=0.69\n",
      "\t [VAL] AP=0.169; [TEST] AP=0.125\n",
      "[10.655s] Iter=29, train loss=0.69\n",
      "\t [VAL] AP=0.165; [TEST] AP=0.127\n",
      "[10.953s] Iter=30, train loss=0.69\n",
      "\t [VAL] AP=0.165; [TEST] AP=0.133\n",
      "[10.748s] Iter=31, train loss=0.69\n",
      "\t [VAL] AP=0.164; [TEST] AP=0.137\n",
      "[10.52s] Iter=32, train loss=0.689\n",
      "\t [VAL] AP=0.162; [TEST] AP=0.137\n",
      "[10.541s] Iter=33, train loss=0.689\n",
      "\t [VAL] AP=0.164; [TEST] AP=0.133\n",
      "[10.702s] Iter=34, train loss=0.689\n",
      "\t [VAL] AP=0.156; [TEST] AP=0.131\n",
      "[10.256s] Iter=35, train loss=0.689\n",
      "\t [VAL] AP=0.16; [TEST] AP=0.124\n",
      "[10.981s] Iter=36, train loss=0.689\n",
      "\t [VAL] AP=0.156; [TEST] AP=0.13\n",
      "[10.971s] Iter=37, train loss=0.689\n",
      "\t [VAL] AP=0.16; [TEST] AP=0.122\n",
      "[10.759s] Iter=38, train loss=0.689\n",
      "\t [VAL] AP=0.156; [TEST] AP=0.131\n",
      "[10.549s] Iter=39, train loss=0.689\n",
      "\t [VAL] AP=0.162; [TEST] AP=0.122\n",
      "[10.675s] Iter=40, train loss=0.689\n",
      "\t [VAL] AP=0.166; [TEST] AP=0.124\n",
      "[10.703s] Iter=41, train loss=0.689\n",
      "\t [VAL] AP=0.166; [TEST] AP=0.134\n",
      "[10.456s] Iter=42, train loss=0.689\n",
      "\t [VAL] AP=0.158; [TEST] AP=0.126\n",
      "[10.409s] Iter=43, train loss=0.689\n",
      "\t [VAL] AP=0.165; [TEST] AP=0.131\n",
      "[10.72s] Iter=44, train loss=0.689\n",
      "\t [VAL] AP=0.16; [TEST] AP=0.127\n",
      "[10.69s] Iter=45, train loss=0.689\n",
      "\t [VAL] AP=0.172; [TEST] AP=0.134\n",
      "[10.642s] Iter=46, train loss=0.689\n",
      "\t [VAL] AP=0.162; [TEST] AP=0.13\n",
      "[10.43s] Iter=47, train loss=0.689\n",
      "\t [VAL] AP=0.165; [TEST] AP=0.124\n",
      "[10.523s] Iter=48, train loss=0.689\n",
      "\t [VAL] AP=0.177; [TEST] AP=0.133\n",
      "[10.639s] Iter=49, train loss=0.689\n",
      "\t [VAL] AP=0.164; [TEST] AP=0.122\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbConcat --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001 --use_numeric True --use_best_feats True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=32, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbDotProd', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=False, use_cuda=True, use_numeric=False, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 0, 'k_dim_field': 4, 'k_dim_id': 32, 'need_activation': True}\n",
      "MODEL: LinEmbDotProd(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 32)\n",
      "  (emb_horse): Embedding(4399, 32)\n",
      "  (emb_trainer): Embedding(129, 32)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (Linear_field): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (Linear_dr): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[16.378s] Iter=0, train loss=0.696\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.081\n",
      "[15.405s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.092; [TEST] AP=0.086\n",
      "[15.56s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.095; [TEST] AP=0.087\n",
      "[15.456s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.094; [TEST] AP=0.085\n",
      "[15.196s] Iter=4, train loss=0.693\n",
      "\t [VAL] AP=0.093; [TEST] AP=0.085\n",
      "[15.613s] Iter=5, train loss=0.693\n",
      "\t [VAL] AP=0.093; [TEST] AP=0.085\n",
      "[15.607s] Iter=6, train loss=0.693\n",
      "\t [VAL] AP=0.093; [TEST] AP=0.085\n",
      "[15.475s] Iter=7, train loss=0.693\n",
      "\t [VAL] AP=0.092; [TEST] AP=0.086\n",
      "[15.489s] Iter=8, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.087\n",
      "[15.519s] Iter=9, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.089\n",
      "[15.666s] Iter=10, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.09\n",
      "[15.606s] Iter=11, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.09\n",
      "[15.474s] Iter=12, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.09\n",
      "[15.655s] Iter=13, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.09\n",
      "[15.378s] Iter=14, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.09\n",
      "[15.745s] Iter=15, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.09\n",
      "[15.697s] Iter=16, train loss=0.693\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.09\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./horse/train.py\", line 233, in <module>\n",
      "    ep_loss.append(loss.data.to(device('cpu')).tolist())\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbDotProd --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=32, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbDotProd', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=False, use_cuda=True, use_numeric=True, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 109, 'k_dim_field': 4, 'k_dim_id': 32, 'need_activation': True}\n",
      "MODEL: LinEmbDotProd(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 32)\n",
      "  (emb_horse): Embedding(4399, 32)\n",
      "  (emb_trainer): Embedding(129, 32)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=113, out_features=1, bias=True)\n",
      "  (Linear_field): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (Linear_dr): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[16.312s] Iter=0, train loss=0.695\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.084\n",
      "[15.518s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.084; [TEST] AP=0.085\n",
      "[15.382s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.085\n",
      "[15.423s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.087\n",
      "[15.672s] Iter=4, train loss=0.693\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.084\n",
      "[15.821s] Iter=5, train loss=0.693\n",
      "\t [VAL] AP=0.084; [TEST] AP=0.079\n",
      "[15.38s] Iter=6, train loss=0.693\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.08\n",
      "[15.569s] Iter=7, train loss=0.693\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.078\n",
      "[15.413s] Iter=8, train loss=0.693\n",
      "\t [VAL] AP=0.084; [TEST] AP=0.081\n",
      "[15.679s] Iter=9, train loss=0.693\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.083\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./horse/train.py\", line 231, in <module>\n",
      "    opt.step()\n",
      "  File \"/userhome/cs2/shenghui/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 140, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/userhome/cs2/shenghui/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/userhome/cs2/shenghui/anaconda3/lib/python3.8/site-packages/torch/optim/adamw.py\", line 162, in step\n",
      "    adamw(params_with_grad,\n",
      "  File \"/userhome/cs2/shenghui/anaconda3/lib/python3.8/site-packages/torch/optim/adamw.py\", line 219, in adamw\n",
      "    func(params,\n",
      "  File \"/userhome/cs2/shenghui/anaconda3/lib/python3.8/site-packages/torch/optim/adamw.py\", line 316, in _single_tensor_adamw\n",
      "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbDotProd --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001 --use_numeric True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=32, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbDotProd', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=True, use_cuda=True, use_numeric=True, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 22, 'k_dim_field': 4, 'k_dim_id': 32, 'need_activation': True}\n",
      "MODEL: LinEmbDotProd(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 32)\n",
      "  (emb_horse): Embedding(4399, 32)\n",
      "  (emb_trainer): Embedding(129, 32)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=26, out_features=1, bias=True)\n",
      "  (Linear_field): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (Linear_dr): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./horse/train.py\", line 230, in <module>\n",
      "    loss.backward()\n",
      "  File \"/userhome/cs2/shenghui/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/userhome/cs2/shenghui/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbDotProd --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001 --use_numeric True --use_best_feats True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=64, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbElemProd', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=False, use_cuda=True, use_numeric=False, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 0, 'k_dim_field': 4, 'k_dim_id': 64, 'need_activation': True}\n",
      "MODEL: LinEmbElemProd(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 64)\n",
      "  (emb_horse): Embedding(4399, 64)\n",
      "  (emb_trainer): Embedding(129, 64)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=136, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[11.78s] Iter=0, train loss=0.696\n",
      "\t [VAL] AP=0.11; [TEST] AP=0.083\n",
      "[10.769s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.117; [TEST] AP=0.081\n",
      "[11.114s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.117; [TEST] AP=0.075\n",
      "[10.576s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.114; [TEST] AP=0.088\n",
      "[10.782s] Iter=4, train loss=0.693\n",
      "\t [VAL] AP=0.104; [TEST] AP=0.071\n",
      "[10.771s] Iter=5, train loss=0.692\n",
      "\t [VAL] AP=0.127; [TEST] AP=0.073\n",
      "[10.597s] Iter=6, train loss=0.691\n",
      "\t [VAL] AP=0.133; [TEST] AP=0.092\n",
      "[10.859s] Iter=7, train loss=0.691\n",
      "\t [VAL] AP=0.131; [TEST] AP=0.092\n",
      "[10.802s] Iter=8, train loss=0.69\n",
      "\t [VAL] AP=0.138; [TEST] AP=0.098\n",
      "[10.734s] Iter=9, train loss=0.69\n",
      "\t [VAL] AP=0.152; [TEST] AP=0.1\n",
      "[10.808s] Iter=10, train loss=0.69\n",
      "\t [VAL] AP=0.144; [TEST] AP=0.104\n",
      "[10.875s] Iter=11, train loss=0.689\n",
      "\t [VAL] AP=0.139; [TEST] AP=0.106\n",
      "[10.994s] Iter=12, train loss=0.689\n",
      "\t [VAL] AP=0.152; [TEST] AP=0.106\n",
      "[10.654s] Iter=13, train loss=0.688\n",
      "\t [VAL] AP=0.157; [TEST] AP=0.105\n",
      "[10.702s] Iter=14, train loss=0.688\n",
      "\t [VAL] AP=0.154; [TEST] AP=0.105\n",
      "[10.633s] Iter=15, train loss=0.688\n",
      "\t [VAL] AP=0.152; [TEST] AP=0.105\n",
      "[10.59s] Iter=16, train loss=0.688\n",
      "\t [VAL] AP=0.154; [TEST] AP=0.108\n",
      "[10.931s] Iter=17, train loss=0.688\n",
      "\t [VAL] AP=0.16; [TEST] AP=0.103\n",
      "[10.902s] Iter=18, train loss=0.688\n",
      "\t [VAL] AP=0.148; [TEST] AP=0.102\n",
      "[10.628s] Iter=19, train loss=0.688\n",
      "\t [VAL] AP=0.149; [TEST] AP=0.103\n",
      "[10.619s] Iter=20, train loss=0.688\n",
      "\t [VAL] AP=0.141; [TEST] AP=0.103\n",
      "[10.711s] Iter=21, train loss=0.687\n",
      "\t [VAL] AP=0.145; [TEST] AP=0.103\n",
      "[11.037s] Iter=22, train loss=0.688\n",
      "\t [VAL] AP=0.163; [TEST] AP=0.109\n",
      "[11.025s] Iter=23, train loss=0.687\n",
      "\t [VAL] AP=0.155; [TEST] AP=0.108\n",
      "[10.85s] Iter=24, train loss=0.687\n",
      "\t [VAL] AP=0.154; [TEST] AP=0.106\n",
      "[10.889s] Iter=25, train loss=0.687\n",
      "\t [VAL] AP=0.152; [TEST] AP=0.105\n",
      "[10.706s] Iter=26, train loss=0.687\n",
      "\t [VAL] AP=0.148; [TEST] AP=0.106\n",
      "[10.538s] Iter=27, train loss=0.687\n",
      "\t [VAL] AP=0.152; [TEST] AP=0.111\n",
      "[10.694s] Iter=28, train loss=0.687\n",
      "\t [VAL] AP=0.148; [TEST] AP=0.107\n",
      "[11.021s] Iter=29, train loss=0.687\n",
      "\t [VAL] AP=0.146; [TEST] AP=0.108\n",
      "[11.111s] Iter=30, train loss=0.687\n",
      "\t [VAL] AP=0.155; [TEST] AP=0.107\n",
      "[11.175s] Iter=31, train loss=0.687\n",
      "\t [VAL] AP=0.146; [TEST] AP=0.105\n",
      "[10.908s] Iter=32, train loss=0.687\n",
      "\t [VAL] AP=0.159; [TEST] AP=0.105\n",
      "[10.774s] Iter=33, train loss=0.687\n",
      "\t [VAL] AP=0.155; [TEST] AP=0.112\n",
      "[10.834s] Iter=34, train loss=0.687\n",
      "\t [VAL] AP=0.159; [TEST] AP=0.115\n",
      "[10.787s] Iter=35, train loss=0.687\n",
      "\t [VAL] AP=0.156; [TEST] AP=0.111\n",
      "[10.704s] Iter=36, train loss=0.687\n",
      "\t [VAL] AP=0.16; [TEST] AP=0.109\n",
      "[10.852s] Iter=37, train loss=0.687\n",
      "\t [VAL] AP=0.154; [TEST] AP=0.108\n",
      "[10.942s] Iter=38, train loss=0.687\n",
      "\t [VAL] AP=0.15; [TEST] AP=0.109\n",
      "[11.363s] Iter=39, train loss=0.687\n",
      "\t [VAL] AP=0.154; [TEST] AP=0.112\n",
      "[10.913s] Iter=40, train loss=0.687\n",
      "\t [VAL] AP=0.152; [TEST] AP=0.113\n",
      "[11.328s] Iter=41, train loss=0.687\n",
      "\t [VAL] AP=0.151; [TEST] AP=0.108\n",
      "[10.896s] Iter=42, train loss=0.687\n",
      "\t [VAL] AP=0.152; [TEST] AP=0.112\n",
      "[10.825s] Iter=43, train loss=0.687\n",
      "\t [VAL] AP=0.143; [TEST] AP=0.111\n",
      "[11.105s] Iter=44, train loss=0.687\n",
      "\t [VAL] AP=0.135; [TEST] AP=0.11\n",
      "[10.747s] Iter=45, train loss=0.687\n",
      "\t [VAL] AP=0.143; [TEST] AP=0.111\n",
      "[10.908s] Iter=46, train loss=0.687\n",
      "\t [VAL] AP=0.143; [TEST] AP=0.107\n",
      "[11.023s] Iter=47, train loss=0.687\n",
      "\t [VAL] AP=0.138; [TEST] AP=0.108\n",
      "[11.19s] Iter=48, train loss=0.687\n",
      "\t [VAL] AP=0.148; [TEST] AP=0.11\n",
      "[11.614s] Iter=49, train loss=0.687\n",
      "\t [VAL] AP=0.132; [TEST] AP=0.111\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbElemProd --k_dim_field 4 --k_dim_id 64 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=64, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbElemProd', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=False, use_cuda=True, use_numeric=True, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 109, 'k_dim_field': 4, 'k_dim_id': 64, 'need_activation': True}\n",
      "MODEL: LinEmbElemProd(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 64)\n",
      "  (emb_horse): Embedding(4399, 64)\n",
      "  (emb_trainer): Embedding(129, 64)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=245, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[12.567s] Iter=0, train loss=0.694\n",
      "\t [VAL] AP=0.032; [TEST] AP=0.042\n",
      "[11.869s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.032; [TEST] AP=0.044\n",
      "[11.878s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.042; [TEST] AP=0.05\n",
      "[11.625s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.072; [TEST] AP=0.058\n",
      "[11.645s] Iter=4, train loss=0.692\n",
      "\t [VAL] AP=0.047; [TEST] AP=0.067\n",
      "[11.808s] Iter=5, train loss=0.692\n",
      "\t [VAL] AP=0.072; [TEST] AP=0.073\n",
      "[11.733s] Iter=6, train loss=0.691\n",
      "\t [VAL] AP=0.074; [TEST] AP=0.06\n",
      "[11.512s] Iter=7, train loss=0.691\n",
      "\t [VAL] AP=0.057; [TEST] AP=0.046\n",
      "[11.528s] Iter=8, train loss=0.69\n",
      "\t [VAL] AP=0.081; [TEST] AP=0.05\n",
      "[11.797s] Iter=9, train loss=0.69\n",
      "\t [VAL] AP=0.074; [TEST] AP=0.052\n",
      "[11.767s] Iter=10, train loss=0.689\n",
      "\t [VAL] AP=0.087; [TEST] AP=0.056\n",
      "[11.833s] Iter=11, train loss=0.689\n",
      "\t [VAL] AP=0.088; [TEST] AP=0.06\n",
      "[11.818s] Iter=12, train loss=0.689\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.053\n",
      "[11.595s] Iter=13, train loss=0.689\n",
      "\t [VAL] AP=0.104; [TEST] AP=0.063\n",
      "[11.662s] Iter=14, train loss=0.688\n",
      "\t [VAL] AP=0.112; [TEST] AP=0.065\n",
      "[11.913s] Iter=15, train loss=0.688\n",
      "\t [VAL] AP=0.131; [TEST] AP=0.079\n",
      "[11.802s] Iter=16, train loss=0.688\n",
      "\t [VAL] AP=0.122; [TEST] AP=0.076\n",
      "[11.788s] Iter=17, train loss=0.687\n",
      "\t [VAL] AP=0.116; [TEST] AP=0.06\n",
      "[11.438s] Iter=18, train loss=0.687\n",
      "\t [VAL] AP=0.106; [TEST] AP=0.059\n",
      "[11.54s] Iter=19, train loss=0.687\n",
      "\t [VAL] AP=0.093; [TEST] AP=0.049\n",
      "[11.788s] Iter=20, train loss=0.687\n",
      "\t [VAL] AP=0.089; [TEST] AP=0.057\n",
      "[11.689s] Iter=21, train loss=0.687\n",
      "\t [VAL] AP=0.107; [TEST] AP=0.061\n",
      "[11.842s] Iter=22, train loss=0.687\n",
      "\t [VAL] AP=0.103; [TEST] AP=0.056\n",
      "[11.733s] Iter=23, train loss=0.687\n",
      "\t [VAL] AP=0.112; [TEST] AP=0.064\n",
      "[11.264s] Iter=24, train loss=0.687\n",
      "\t [VAL] AP=0.098; [TEST] AP=0.064\n",
      "[11.965s] Iter=25, train loss=0.687\n",
      "\t [VAL] AP=0.106; [TEST] AP=0.061\n",
      "[11.759s] Iter=26, train loss=0.687\n",
      "\t [VAL] AP=0.09; [TEST] AP=0.056\n",
      "[11.59s] Iter=27, train loss=0.687\n",
      "\t [VAL] AP=0.091; [TEST] AP=0.06\n",
      "[11.756s] Iter=28, train loss=0.687\n",
      "\t [VAL] AP=0.087; [TEST] AP=0.061\n",
      "[11.732s] Iter=29, train loss=0.686\n",
      "\t [VAL] AP=0.101; [TEST] AP=0.064\n",
      "[11.544s] Iter=30, train loss=0.686\n",
      "\t [VAL] AP=0.101; [TEST] AP=0.072\n",
      "[11.755s] Iter=31, train loss=0.686\n",
      "\t [VAL] AP=0.099; [TEST] AP=0.071\n",
      "[11.767s] Iter=32, train loss=0.686\n",
      "\t [VAL] AP=0.08; [TEST] AP=0.057\n",
      "[11.775s] Iter=33, train loss=0.686\n",
      "\t [VAL] AP=0.098; [TEST] AP=0.064\n",
      "[11.844s] Iter=34, train loss=0.686\n",
      "\t [VAL] AP=0.09; [TEST] AP=0.06\n",
      "[11.976s] Iter=35, train loss=0.686\n",
      "\t [VAL] AP=0.087; [TEST] AP=0.064\n",
      "[11.694s] Iter=36, train loss=0.686\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.059\n",
      "[11.804s] Iter=37, train loss=0.686\n",
      "\t [VAL] AP=0.094; [TEST] AP=0.06\n",
      "[11.958s] Iter=38, train loss=0.686\n",
      "\t [VAL] AP=0.07; [TEST] AP=0.056\n",
      "[11.718s] Iter=39, train loss=0.686\n",
      "\t [VAL] AP=0.075; [TEST] AP=0.059\n",
      "[11.869s] Iter=40, train loss=0.686\n",
      "\t [VAL] AP=0.082; [TEST] AP=0.06\n",
      "[11.668s] Iter=41, train loss=0.686\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.062\n",
      "[11.873s] Iter=42, train loss=0.686\n",
      "\t [VAL] AP=0.098; [TEST] AP=0.071\n",
      "[11.843s] Iter=43, train loss=0.686\n",
      "\t [VAL] AP=0.089; [TEST] AP=0.066\n",
      "[11.834s] Iter=44, train loss=0.686\n",
      "\t [VAL] AP=0.094; [TEST] AP=0.065\n",
      "[11.818s] Iter=45, train loss=0.686\n",
      "\t [VAL] AP=0.09; [TEST] AP=0.073\n",
      "[11.746s] Iter=46, train loss=0.686\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.069\n",
      "[11.625s] Iter=47, train loss=0.686\n",
      "\t [VAL] AP=0.093; [TEST] AP=0.069\n",
      "[11.509s] Iter=48, train loss=0.686\n",
      "\t [VAL] AP=0.089; [TEST] AP=0.059\n",
      "[11.71s] Iter=49, train loss=0.686\n",
      "\t [VAL] AP=0.09; [TEST] AP=0.066\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbElemProd --k_dim_field 4 --k_dim_id 64 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001 --use_numeric True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=64, layer_size_reduction=0.5, learning_rate=0.01, model_name='LinEmbElemProd', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=True, use_cuda=True, use_numeric=True, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 22, 'k_dim_field': 4, 'k_dim_id': 64, 'need_activation': True}\n",
      "MODEL: LinEmbElemProd(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 64)\n",
      "  (emb_horse): Embedding(4399, 64)\n",
      "  (emb_trainer): Embedding(129, 64)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=158, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "[12.386s] Iter=0, train loss=0.695\n",
      "\t [VAL] AP=0.055; [TEST] AP=0.071\n",
      "[11.802s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.061; [TEST] AP=0.073\n",
      "[11.664s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.078; [TEST] AP=0.071\n",
      "[11.684s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.108; [TEST] AP=0.11\n",
      "[11.79s] Iter=4, train loss=0.693\n",
      "\t [VAL] AP=0.136; [TEST] AP=0.135\n",
      "[11.731s] Iter=5, train loss=0.692\n",
      "\t [VAL] AP=0.133; [TEST] AP=0.16\n",
      "[11.752s] Iter=6, train loss=0.691\n",
      "\t [VAL] AP=0.123; [TEST] AP=0.156\n",
      "[11.652s] Iter=7, train loss=0.69\n",
      "\t [VAL] AP=0.127; [TEST] AP=0.152\n",
      "[11.746s] Iter=8, train loss=0.69\n",
      "\t [VAL] AP=0.133; [TEST] AP=0.148\n",
      "[11.803s] Iter=9, train loss=0.689\n",
      "\t [VAL] AP=0.125; [TEST] AP=0.152\n",
      "[11.712s] Iter=10, train loss=0.688\n",
      "\t [VAL] AP=0.121; [TEST] AP=0.133\n",
      "[11.73s] Iter=11, train loss=0.688\n",
      "\t [VAL] AP=0.119; [TEST] AP=0.123\n",
      "[11.534s] Iter=12, train loss=0.687\n",
      "\t [VAL] AP=0.104; [TEST] AP=0.125\n",
      "[11.641s] Iter=13, train loss=0.687\n",
      "\t [VAL] AP=0.122; [TEST] AP=0.116\n",
      "[11.861s] Iter=14, train loss=0.687\n",
      "\t [VAL] AP=0.115; [TEST] AP=0.139\n",
      "[11.699s] Iter=15, train loss=0.686\n",
      "\t [VAL] AP=0.115; [TEST] AP=0.138\n",
      "[11.656s] Iter=16, train loss=0.686\n",
      "\t [VAL] AP=0.113; [TEST] AP=0.108\n",
      "[11.56s] Iter=17, train loss=0.686\n",
      "\t [VAL] AP=0.103; [TEST] AP=0.108\n",
      "[11.349s] Iter=18, train loss=0.686\n",
      "\t [VAL] AP=0.12; [TEST] AP=0.125\n",
      "[11.666s] Iter=19, train loss=0.686\n",
      "\t [VAL] AP=0.111; [TEST] AP=0.115\n",
      "[11.845s] Iter=20, train loss=0.685\n",
      "\t [VAL] AP=0.101; [TEST] AP=0.095\n",
      "[11.62s] Iter=21, train loss=0.685\n",
      "\t [VAL] AP=0.103; [TEST] AP=0.106\n",
      "[11.752s] Iter=22, train loss=0.685\n",
      "\t [VAL] AP=0.103; [TEST] AP=0.099\n",
      "[11.724s] Iter=23, train loss=0.685\n",
      "\t [VAL] AP=0.1; [TEST] AP=0.103\n",
      "[11.955s] Iter=24, train loss=0.685\n",
      "\t [VAL] AP=0.099; [TEST] AP=0.095\n",
      "[11.879s] Iter=25, train loss=0.685\n",
      "\t [VAL] AP=0.086; [TEST] AP=0.087\n",
      "[11.693s] Iter=26, train loss=0.685\n",
      "\t [VAL] AP=0.069; [TEST] AP=0.096\n",
      "[11.795s] Iter=27, train loss=0.685\n",
      "\t [VAL] AP=0.077; [TEST] AP=0.092\n",
      "[11.604s] Iter=28, train loss=0.684\n",
      "\t [VAL] AP=0.078; [TEST] AP=0.093\n",
      "[11.687s] Iter=29, train loss=0.684\n",
      "\t [VAL] AP=0.089; [TEST] AP=0.097\n",
      "[11.811s] Iter=30, train loss=0.684\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.101\n",
      "[11.68s] Iter=31, train loss=0.684\n",
      "\t [VAL] AP=0.081; [TEST] AP=0.091\n",
      "[11.87s] Iter=32, train loss=0.684\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.097\n",
      "[11.786s] Iter=33, train loss=0.684\n",
      "\t [VAL] AP=0.071; [TEST] AP=0.094\n",
      "[11.829s] Iter=34, train loss=0.684\n",
      "\t [VAL] AP=0.067; [TEST] AP=0.087\n",
      "[11.634s] Iter=35, train loss=0.684\n",
      "\t [VAL] AP=0.075; [TEST] AP=0.088\n",
      "[11.46s] Iter=36, train loss=0.684\n",
      "\t [VAL] AP=0.077; [TEST] AP=0.091\n",
      "[11.932s] Iter=37, train loss=0.684\n",
      "\t [VAL] AP=0.067; [TEST] AP=0.085\n",
      "[11.851s] Iter=38, train loss=0.684\n",
      "\t [VAL] AP=0.067; [TEST] AP=0.082\n",
      "[11.548s] Iter=39, train loss=0.684\n",
      "\t [VAL] AP=0.075; [TEST] AP=0.08\n",
      "[11.686s] Iter=40, train loss=0.684\n",
      "\t [VAL] AP=0.072; [TEST] AP=0.087\n",
      "[11.667s] Iter=41, train loss=0.683\n",
      "\t [VAL] AP=0.066; [TEST] AP=0.085\n",
      "[11.922s] Iter=42, train loss=0.684\n",
      "\t [VAL] AP=0.071; [TEST] AP=0.08\n",
      "[11.654s] Iter=43, train loss=0.684\n",
      "\t [VAL] AP=0.071; [TEST] AP=0.083\n",
      "[11.716s] Iter=44, train loss=0.684\n",
      "\t [VAL] AP=0.071; [TEST] AP=0.086\n",
      "[11.782s] Iter=45, train loss=0.683\n",
      "\t [VAL] AP=0.068; [TEST] AP=0.089\n",
      "[11.807s] Iter=46, train loss=0.684\n",
      "\t [VAL] AP=0.073; [TEST] AP=0.081\n",
      "[11.863s] Iter=47, train loss=0.683\n",
      "\t [VAL] AP=0.074; [TEST] AP=0.09\n",
      "[11.616s] Iter=48, train loss=0.683\n",
      "\t [VAL] AP=0.077; [TEST] AP=0.082\n",
      "[11.59s] Iter=49, train loss=0.683\n",
      "\t [VAL] AP=0.079; [TEST] AP=0.076\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name LinEmbElemProd --k_dim_field 4 --k_dim_id 64 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001 --use_numeric True --use_best_feats True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTS: Namespace(batch_size=20, dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, epoch=50, k_dim_field=4, k_dim_id=32, layer_size_reduction=0.5, learning_rate=0.01, model_name='EmbMLP', num_layers=2, out_src='./output', p_dropout=0.1, test_size=0.1, train_size=0.8, use_best_feats=False, use_cuda=True, use_numeric=False, val_size=0.1, weight_decay=0.0001)\n",
      "\n",
      "Model param: {'n_dr': 15, 'n_field': 9, 'n_jockey': 131, 'n_horse': 4399, 'n_trainer': 129, 'n_num_feats': 0, 'k_dim_field': 4, 'k_dim_id': 32, 'need_activation': True, 'num_layers': 2, 'p_dropout': 0.1}\n",
      "MODEL: EmbMLP(\n",
      "  (emb_dr): Embedding(15, 4)\n",
      "  (emb_field): Embedding(9, 4)\n",
      "  (emb_jockey): Embedding(131, 32)\n",
      "  (emb_horse): Embedding(4399, 32)\n",
      "  (emb_trainer): Embedding(129, 32)\n",
      "  (relu): ReLU()\n",
      "  (Linear): Linear(in_features=26, out_features=1, bias=True)\n",
      "  (MLP_Layer): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=104, out_features=52, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=52, out_features=26, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "[15.836s] Iter=0, train loss=0.693\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.085\n",
      "[15.211s] Iter=1, train loss=0.693\n",
      "\t [VAL] AP=0.095; [TEST] AP=0.096\n",
      "[15.029s] Iter=2, train loss=0.693\n",
      "\t [VAL] AP=0.078; [TEST] AP=0.067\n",
      "[14.871s] Iter=3, train loss=0.693\n",
      "\t [VAL] AP=0.066; [TEST] AP=0.067\n",
      "[14.792s] Iter=4, train loss=0.693\n",
      "\t [VAL] AP=0.074; [TEST] AP=0.073\n",
      "[14.843s] Iter=5, train loss=0.693\n",
      "\t [VAL] AP=0.053; [TEST] AP=0.067\n",
      "[15.129s] Iter=6, train loss=0.693\n",
      "\t [VAL] AP=0.081; [TEST] AP=0.06\n",
      "[14.977s] Iter=7, train loss=0.693\n",
      "\t [VAL] AP=0.077; [TEST] AP=0.061\n",
      "[14.845s] Iter=8, train loss=0.693\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.073\n",
      "[15.243s] Iter=9, train loss=0.693\n",
      "\t [VAL] AP=0.084; [TEST] AP=0.078\n",
      "[15.047s] Iter=10, train loss=0.693\n",
      "\t [VAL] AP=0.093; [TEST] AP=0.069\n",
      "[15.008s] Iter=11, train loss=0.693\n",
      "\t [VAL] AP=0.098; [TEST] AP=0.073\n",
      "[14.573s] Iter=12, train loss=0.693\n",
      "\t [VAL] AP=0.077; [TEST] AP=0.077\n",
      "[15.092s] Iter=13, train loss=0.693\n",
      "\t [VAL] AP=0.087; [TEST] AP=0.083\n",
      "[15.197s] Iter=14, train loss=0.693\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.084\n",
      "[15.053s] Iter=15, train loss=0.693\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.084\n",
      "[15.09s] Iter=16, train loss=0.693\n",
      "\t [VAL] AP=0.084; [TEST] AP=0.084\n",
      "[14.553s] Iter=17, train loss=0.693\n",
      "\t [VAL] AP=0.083; [TEST] AP=0.081\n",
      "[14.704s] Iter=18, train loss=0.693\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.084\n",
      "[15.047s] Iter=19, train loss=0.693\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.084\n",
      "[14.955s] Iter=20, train loss=0.693\n",
      "\t [VAL] AP=0.084; [TEST] AP=0.084\n",
      "[14.881s] Iter=21, train loss=0.693\n",
      "\t [VAL] AP=0.084; [TEST] AP=0.084\n",
      "[14.985s] Iter=22, train loss=0.693\n",
      "\t [VAL] AP=0.085; [TEST] AP=0.084\n",
      "[14.955s] Iter=23, train loss=0.693\n",
      "\t [VAL] AP=0.084; [TEST] AP=0.084\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/train.py --model_name EmbMLP --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./horse/train.py --model_name EmbMLP --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001 --use_numeric True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./horse/train.py --model_name EmbMLP --k_dim_field 4 --k_dim_id 32 --epoch 50 --batch_size 20 --learning_rate 0.01 --weight_decay 0.0001 --use_numeric True --use_best_feats True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(C=0.5, algorithm='SAMME.R', alpha=1.0, base_estimator='deprecated', colsample_bytree=0.8, criterion='entropy', dataset_path='./horse/data/perform_full_feature.csv', do_categorization=True, do_scale=True, eval_metric='mae', if_odds=False, learning_rate=0.1, loss='linear', max_depth=10, max_iter=1000, min_child_weight=1, min_samples_leaf=10, min_samples_split=10, model='logistic', n_estimators=50, objective='binary:logistic', penalty='l1', random_state=8017, reg_alpha=0.0001, reg_lambda=0.0001, solver='liblinear', splitter='random', subsample=0.8, target='is_champ', test_size=0.1, train_size=0.8, tree_method='gpu_hist', use_best_feats=True, val_size=0.1)\n",
      "Index(['race_key', 'race_date', 'race_money', 'horse_bestperform_h',\n",
      "       'horse_champ_rate_h', 'horse_champs_h', 'horse_life_time_d',\n",
      "       'horse_place_m', 'horse_top4_rate_y', 'horse_top4_h',\n",
      "       'horse_top4_last5', 'jockey_champ_rate_h', 'jockey_champ_last5',\n",
      "       'jockey_champ_y', 'jockey_champ_rate_y', 'jockey_elo',\n",
      "       'jockey_top4_rate_y', 'jockey_top4_m', 'jockey_top4_rate_m',\n",
      "       'trainer_champ_rate_h', 'trainer_champ_rate_y', 'trainer_place_rate_h',\n",
      "       'trainer_top4_rate_y', 'trainer_top4_rate_m', 'is_champ', 'pla', 'dr'],\n",
      "      dtype='object')\n",
      "[0.663 s] DONE logistic.\n",
      "AP for logistic: 0.303\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "!python ./horse/ml_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_key</th>\n",
       "      <th>race_date</th>\n",
       "      <th>dr</th>\n",
       "      <th>distance</th>\n",
       "      <th>field_going</th>\n",
       "      <th>course_type</th>\n",
       "      <th>race_money</th>\n",
       "      <th>act_wt</th>\n",
       "      <th>declare_horse_wt</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>...</th>\n",
       "      <th>horse_year_place_rate</th>\n",
       "      <th>horse_year_top4_rate</th>\n",
       "      <th>num_games_year</th>\n",
       "      <th>horse_month_champ</th>\n",
       "      <th>horse_month_place</th>\n",
       "      <th>horse_month_top4</th>\n",
       "      <th>num_games_month</th>\n",
       "      <th>horse_month_champ_rate</th>\n",
       "      <th>horse_month_place_rate</th>\n",
       "      <th>horse_month_top4_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/4/22</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>575000</td>\n",
       "      <td>120</td>\n",
       "      <td>1186</td>\n",
       "      <td>7.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/4/22</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>575000</td>\n",
       "      <td>132</td>\n",
       "      <td>1022</td>\n",
       "      <td>6.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/4/22</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>575000</td>\n",
       "      <td>121</td>\n",
       "      <td>1085</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/4/22</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>575000</td>\n",
       "      <td>127</td>\n",
       "      <td>1211</td>\n",
       "      <td>7.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015/04/22_1</td>\n",
       "      <td>2015/4/22</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>575000</td>\n",
       "      <td>124</td>\n",
       "      <td>1088</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       race_key  race_date  dr  distance field_going course_type  race_money  \\\n",
       "0  2015/04/22_1  2015/4/22   2      1000                       575000   \n",
       "1  2015/04/22_1  2015/4/22   3      1000                       575000   \n",
       "2  2015/04/22_1  2015/4/22   5      1000                       575000   \n",
       "3  2015/04/22_1  2015/4/22   7      1000                       575000   \n",
       "4  2015/04/22_1  2015/4/22   1      1000                       575000   \n",
       "\n",
       "   act_wt  declare_horse_wt  win_odds  ... horse_year_place_rate  \\\n",
       "0     120              1186       7.3  ...                   0.0   \n",
       "1     132              1022       6.1  ...                   0.0   \n",
       "2     121              1085      48.0  ...                   0.0   \n",
       "3     127              1211       7.8  ...                   0.0   \n",
       "4     124              1088      14.0  ...                   0.0   \n",
       "\n",
       "  horse_year_top4_rate num_games_year  horse_month_champ  horse_month_place  \\\n",
       "0                  0.0              0                  0                  0   \n",
       "1                  0.0              0                  0                  0   \n",
       "2                  0.0              0                  0                  0   \n",
       "3                  0.0              0                  0                  0   \n",
       "4                  0.0              0                  0                  0   \n",
       "\n",
       "   horse_month_top4  num_games_month  horse_month_champ_rate  \\\n",
       "0                 0                0                     0.0   \n",
       "1                 0                0                     0.0   \n",
       "2                 0                0                     0.0   \n",
       "3                 0                0                     0.0   \n",
       "4                 0                0                     0.0   \n",
       "\n",
       "   horse_month_place_rate  horse_month_top4_rate  \n",
       "0                     0.0                    0.0  \n",
       "1                     0.0                    0.0  \n",
       "2                     0.0                    0.0  \n",
       "3                     0.0                    0.0  \n",
       "4                     0.0                    0.0  \n",
       "\n",
       "[5 rows x 126 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('./horse/data/perform_final_feature.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54436 entries, 0 to 54435\n",
      "Columns: 126 entries, race_key to horse_month_top4_rate\n",
      "dtypes: float64(47), int64(72), object(7)\n",
      "memory usage: 52.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
